%%%%%%%%%%%% INTRODUCCION / INTRODUCTION CHAPTER %%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{cha:intro}

\section{Motivation}
\label{sec:motivation}
Interacting with an environment is one of the most intuitive ways to learn, especially for us humans. Since the beginning of time, species have competed against each other and their environment, with a simple rule, those who adapt survive. A very simple way to describe these dynamics is to say that us humans (agents) perform actions in the world (environment), and the world "rewards" those actions. A simple example of this could be the use of fire. If we use it to heat food, it will kill bacteria and other micro-organisms, which may potentially harm us, but if we put our hands in it, it may create bruises in our skin that could result in an infection and a potential death. Generation after generation, our cognitive abilities as a product of evolution have made possible to pass knowledge about how our environment works. By compiling data and experiences about what gives us the most positive rewards (using fire as a beneficial tool) or the most negative (burning ourselves with fire), we have created a knowledge basis that makes us interact with the world in a beneficial way. This set of dynamics and rules can be formalized into a framework called reinforcement learning. Reinforcement learning (RL) is a field of Artificial Intelligence (AI) that describes an agent learns to behave, selecting actions that follow some policy in such a way that maximizes the reward. 

Evolution towards more complex and sophisticated systems for adaptation to an ever-changing environment seems like a pattern in history. At first, very "basic" but understandable kinds of species populated the earth, such as micro-organisms like bacteria or virus, that had "basic" but useful capabilities for extreme conditions resistance such as DNA repair mechanisms or survivals strategies such as spores or cyst formations that, although astonishing, were "simple" molecular reactions that made them survive in the most of hostiles environments. As generations went on, more complex organisms with lots of diverse functionalities such as eyesight or eco-localization appeared. Similarly, one of the most important breakouts in this field of RL was incorporating complex systems to the agent such as deep neural networks to process the observations of an environment. They are used as systems that command the agent behaviour, like some sort of brain that perceives, evaluates and executes. They are of especial use for complex environments where the number of states its unfeasible to represent using only tabular methods. Although neural networks models give a lot of representation power to the agent, it comes with a cost. Most of the times, they are treated as black boxes, given their complexity, and this causes a problem in interpreting the decisions that an agent may perform. Several improvements have been of use to understand what the network is looking at in order to perform an action, such as using convolutional neural networks (CNNs), but their lack of interpretability, as the network goes deeper, ends up in a set of descriptions that are not understandable for the human eye. Several works have tried to interpret the activation of the CNN model, but lots of work is yet to be done in this department, since most of the results are biased towards the supervised set-up.

 The attention mechanism was introduced for the realm of natural language processing (NLP) as a function that characterizes what the network thinks is more important from the input data, and the relations between the elements of the input sequence. The original target for this type of models was to translate an input sentence or perform next-token-prediction. With this functionality, researchers could now see which parts of the input had higher importance (or weight), giving a richer context on how the model "reasons" about the data that is fed. In the supervised learning domain this is quite useful. For example, in a classification task, such as image classification, the network reveals which are the most descriptive features of a class that make the prediction, or in a regression task, such as energy demand prediction or stock market price estimation, where the attention mechanism tells which parts of the historic data were more relevant to come up with the predicted value. 
 
 With this intuition, we propose to use the attention mechanism as a tool for explainability in the decision making of an agent. Our main goal is to use an agent that makes use of attention to evaluate the decision making given the context. For example, in an autonomous driving environment, we could imagine a car going towards a cross-walk when a pedestrian comes across the street. As humans, we focus our attention on the pedestrian, and press the brakes of the car for it to stop. We expect something similar from the agent, as to give us some intuitive visual cues of what it considers important, given an observation of an environment to perform an action.


\section{Objectives}
\label{sec:objectives}
The reinforcement learning problem is a little bit out of the scope of the contents given in this master. This is why, one of the main goals is to clearly understand the reinforcement learning problem and its formalization. We will explore the classical methods, such as Monte Carlo learning or TD-learning, and point out their limitations to understand when does Deep Learning come into play. Then, we will perform some experiments over well established techniques such as Double Deep Q Networks, with the aim to see how different approaches affect the way an agent learns. For this work we do not aim for policy gradients methods, although they remain as future work to further explore the whole realm of RL.

Also, we will need to go deep into the attention mechanism, especially self-attention and its implementation on the transformer model. The architecture behind the transformer is far from trivial, since it comprises concepts from both computer science and linear algebra to produce essential components such as the self attention layer. We will delve into understanding the different kinds of attention that are the state of the art from models such as the vision transformer or the SWIN Transformer.

Finally, with all this knowledge, we will also aim to develop an efficient pipeline that allow us to test several set-ups. Given the nature of the problem, we know that the reinforcement learning algorithms are slow in convergence, especially with function approximators such as neural networks. 

All of this goals will not only serve for understanding theory and put everything in practice, but also to experiment a little bit and extract empirical results. These results will be used for testing wether the attention mechanism can be used as a standalone explanation tool for RL.

\section{Report structure}
In \autoref{cha:intro} we give a brief introduction to the main goals of this work. We briefly explain the problem of explainability in reinforcement learning, and how we think attention can help to solve it.
Following the first chapter, in \autoref{cha:related-work} we go over the starting point of this work, contemplating several approaches to the reinforcement learning problem and how we can tackle it using attention, addressing  previous work as our starting point. A review is done on the main attention models for visual data and how they are incorporated onto the reinforcement learning paradigm.
Once we have explained the basic practical and theoretic components, in \autoref{cha:design-development} we go deep into how we have implement the set-up where we are going to evaluate the explainability of attention-based models. Since the online reinforcement learning set-up does not use a dataset, but rather an interactive environment, we will explain how we connected a model oriented to supervised learning into the reinforcement learning framework.
With our design and development explained, \autoref{cha:evaluation} provides the procedure involved in obtaining our results and the insights that we have extracted from the attention mechanism as an explainability tool for decision-making.
Finally, in \autoref{cha:conclusions} we will sum up all the information and give our main thoughts of this work. We will go over the results and give our view on them, and discuss if they are relevant. Also, given the results, we will explore new lines of work that may be interesting to pursue in order to extract better knowledge from the attention mechanism as an intuitive tool for explainability in RL.


