%%%%%%%%%%%% INTRODUCCION / INTRODUCTION CHAPTER %%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{cha:intro}

\section{Motivation}
\label{sec:motivation}
Interacting with an environment is one of the most intuitive ways to learn, especially for us humans. Since the beginning of time, species have competed against each other and their environment, with a simple rule, those who adapt survive. A very simple way to describe these dynamics is to say that us humans (agents) perform actions in the world (environment), and the world "rewards" those actions. A simple example of this could be the use of fire. If we use it to heat food, it will kill bacteria and other micro-organisms, which may potentially harm us, but if we put our hands in it, it may create bruises in our skin that could result in an infection and a potential death. Generation after generation, our cognitive abilities as a product of evolution have made possible to pass knowledge about how our environment works. By compiling data and experiences about what gives us the most positive rewards (using fire as a beneficial tool) or the most negative (burning ourselves with fire), we have created a knowledge basis that makes us interact with the world in a beneficial way. This set of dynamics and rules can be formalized into a framework called reinforcement learning. Reinforcement learning (RL) is a field of Artificial Intelligence (AI) that describes an agent that learns to behave, selecting actions that follow some policy in such a way that maximize the reward. 

Evolution towards more complex and sophisticated systems for adaptation to an ever-changing environment seems like a pattern in history. At first, very "basic" but understandable kinds of species populated the earth, such as micro-organisms like bacteria or virus, that had "basic" but useful capabilities for extreme conditions resistance such as DNA repair mechanisms or survival strategies such as spores or cyst formations that, although astonishing, were "simple" molecular reactions that made them survive in the most of hostile environments. As generations went on, more complex organisms with lots of diverse functionalities such as eyesight or eco-localization appeared. Similarly, one of the most important breakouts in this field of RL was incorporating complex systems to the agent such as deep neural networks to process the observations of an environment. 

At first, problems of RL were solved in "simple" tabular ways using dynamic programming, but as the number of possible states from an environment grew up, these methods fell short to address the problem, since they required of lots of memory and computational power to solve them. Sutton \textit{et al.} coined this problem as the "curse of dimensionality" \cite{intro_rl}. To tackle this issue, they propose to introduce function approximators (linear and polynomial), but with the growing interest of neural networks in machine learning, researchers opted to use them instead. Neural networks are of especial use for complex environments where the number of states its unfeasible to represent using only tabular methods. They are used as systems that command the agent behaviour, like some sort of brain that perceives, evaluates and executes. Although neural networks models give a lot of representation power to the agent, it comes with a cost. Most of the times, they are treated as black boxes, given their complexity, and this causes a problem in interpreting the decisions that an agent may perform. Several improvements have been of use to understand what the network is looking at in order to perform an action, such as using convolutional neural networks (CNNs), but their lack of interpretability, as the network goes deeper, ends up in a set of descriptions that are not understandable for the human eye. Several works \cite{bach2015pixel, Selvaraju_2019, Chattopadhay_2018, Muhammad_2020} have tried to interpret the activations of the receptive fields from the CNN model, but lots of work is yet to be done in this department, since most of the results are biased towards the supervised set-up.

 The attention mechanism was introduced for the realm of natural language processing (NLP) as a function that characterizes what the network thinks is more important from the input data, and the relations between the elements of the input sequence. The original target for this type of models was to translate an input sentence or perform next-token-prediction. With this functionality, researchers could now see which parts of the input had higher importance (or weight), giving a richer context on how the model "reasons" about the data that is fed. In the supervised learning domain this is quite useful. For example, in a classification task, such as image classification, the network reveals which are the most descriptive features of a class that make the prediction, or in a regression task, such as energy demand prediction or stock market price estimation, where the attention mechanism tells which parts of the historic data were more relevant to come up with the predicted value. 
 
 With this intuition, we propose to use the attention mechanism as a tool for explainability in the decision making of an agent. Our main goal is to use an agent that makes use of attention (along other interpretability and explainability techniques) to evaluate the decision making given the observation of an environment. For example, in an autonomous driving set-up, we could imagine a car going towards a cross-walk when a pedestrian comes across the street. As humans, we focus our attention on the pedestrian, and press the brakes of the car for it to stop. We expect something similar from the agent, as to give us some intuitive visual cues of what it considers important, given an observation of an environment to perform an action.
 
 All the developed code is available in \url{https://github.com/Javimh18/DL_TFM/tree/main}. Some evidences come from some videos that we have extracted from the agents playing games, those videos are available in \url{https://drive.google.com/drive/folders/1cABvE9vmXMyWWHmAqBzeffJOcFBc7GZD?usp=sharing}.


\section{Objectives}
\label{sec:objectives}
\begin{enumerate}
	\item First, we will provide a comprehensive review of the basic time difference methods, such as Monte Carlo, SARSA or Q-learning. This is done to show our understanding on the matter, since it falls a little out of scope of the master's program.
	\item We will implement the different DQN learning algorithms. 
	\item For each model we will implement their own attention mechanism, giving a extensive review of the approaches proposed for the implemented models.
	\item We will test our implementations in classical environments based on the Arcade Learning Environment (ALE)
	\item Implement a pipeline to enable automatic training, evaluation and metrics extraction.
	\item Evaluate the results using attention and other explainability techniques in the RL paradigm.
\end{enumerate}

\section{Report structure}
In \autoref{cha:intro} we give a brief introduction to the main goals of this work. We briefly explain the problem of explainability in reinforcement learning, and how we think attention can help to solve it.
Following the first chapter, in \autoref{cha:related-work} we go over the starting point of this work, contemplating several approaches to the reinforcement learning problem and how we can tackle it using attention, addressing  previous work as our starting point. A review is done on the main attention models for visual data and how they are incorporated onto the reinforcement learning paradigm.
Once we have explained the basic practical and theoretic components, in \autoref{cha:design-development} we go deep into how we have implement the set-up where we are going to evaluate the explainability of attention-based models. Since the online reinforcement learning set-up does not use a dataset, but rather an interactive environment, we will explain how we connected a model oriented to supervised learning into the reinforcement learning framework.
With our design and development explained, \autoref{cha:evaluation} provides the procedure involved in obtaining our results and the insights that we have extracted from the attention mechanism as an explainability tool for decision-making.
Finally, in \autoref{cha:conclusions} we will sum up all the information and give our main thoughts of this work. We will go over the results and give our view on them, and discuss if they are relevant. Also, given the results, we will explore new lines of work that may be interesting to pursue in order to extract better knowledge from the attention mechanism as an intuitive tool for explainability in RL.


