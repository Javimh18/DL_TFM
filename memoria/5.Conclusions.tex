%%%%%%%%%%%%%%%%%%%% CONCLUSIONES Y TRABAJO FUTURO / CONCLUSIONS AND FUTURE WORK CHAPTER %%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions and future work}
\label{cha:conclusions}

\section{Conclusions}
Given the evaluation that we have provided, one thing is clear: Attention can be used as an explainability technique to understand the decisions that an agent may perform. The ViT seems a suitable model to address this issue, as it provides the most explainable features, both when it performs great and when it performs poor. The elephant in the room with this model is its computational complexity, since we have seen that it has the roughly the same number of weights as the CNN agent but orders of magnitude more complexity in the forward pass. The SWIN transformer seems like the most balanced model, since it is not the most complex model, and is efficient computing the attention in a hierarchical fashion, using windows, which reminds to the CNN. The issue with this model is that, since it lacks of a class token, we cannot actually look into which patches or which windows are crucial for the decision on taking some action or another. At last, the CNN model was the most surprising for us, since it held its own in terms of performance, being close to the attention-based models or even surpassing them, as we saw with the DemonAttack enviroment. This model lacks of explainability, which may be a concern for applications of RL in real world scenarios, but it reminds us that sometimes, new is not always better.

Referring to the training schedules, we have concluded that the best performing schedule is actually the exponential decay. As we have stated in section \ref{sec:eval_exp_schedules}, this is for us quite counter-intuitive, and further exploration should be done, to clarify whether what happened is actually true or not.

On a personal note, this work has been quite challenging, since we almost had no prior knowledge of reinforcement learning before we developed this master thesis. By working in some out-of-scope field, we have actually learned a lot on why Deep Learning is useful to tackle not only supervised problems, but also another paradigms as the one we have faced in this work. We do not know if RL is the way to go to achieve artificial general intelligence, but it sure does has it similarities with humans, and if we actually managed to make it understandable for us, who knows, maybe we could understand what is missing to actually make them behave like us, or even better.

\section{Future work}

Lots of work can potentially be done to continue exploring the area of explainability in the area of reinforcement learning. First, the results were obtained by restraining the total time-steps to almost a quarter of the usual set-up in the literature, so it would be of great use to train the models up to completion to have better insights on the actual performance of the model. With a well-trained model, we argue that the attention and activation maps would have more relevance, since we would be analysing the behaviour of a "almost" optimal agent. 

To speed-up the training procedures, we could implement a pipeline that makes use of distributed GPU computing. In the state of the art's frameworks for Deep Learning, such as TensorFlow or PyTorch, distributed and paralleled training is supported supervised and semi-supervised set-ups, but not for reinforcement learning. We have seen in this work that there have been some advancements as there are developed libraries where this is implemented using Message Passing Interfaces. We believe that developing an efficient parallelized pipeline would improve training times by a lot, making room for more experiments that would give us better insights on the proposed models.

Since we have trained effectively value functions, we could use them for training Actor-Critic methods. As explained, these methods use two models, a policy model, that governs the agent's behaviour and performs actions, and a value function that evaluates how good those actions are. We already have a value function, and we are able to extract explanations from them, so we argue that we could extend its use to Actor-Critic set-ups for better understanding on the developed policy. Additionally, the ViT-Big model was not trained due to time constrains, but we wonder whether it would outperform all the proposed models, giving a even better Q-network to estimate the values from states.

For the explainability and interpretability in attention models, we have shown that attention methods do explain the agent's behaviour to an extent, but sometimes fall sort, since attention maps are less expressive than other methods. In the last years, Meta AI has discovered that in a self-supervised set-up using distillation \cite{hinton2015distilling} and mean student-teacher training \cite{tarvainen2018mean}, where quite interesting properties emerge in the attention maps. This model is called DINO \cite{caron2021emerging}, and we argue that by using transfer learning and adapting the pre-trained weights to our problem, some interesting findings could arise, such as clearer attention maps or faster convergence to an better policy behaviour. Additionally, there is a family of class activation methods (CAM) \cite{layerCAM, fu2020axiombased, Chattopadhay_2018, Muhammad_2020} that we could also try to explore if some relevant findings appear in the activation maps, or use Layer-wise Relevance Propagation and its variants to extract its activation maps and see if some better explanations are provided, since these methods do not need of a up-scaling as CAM methods do, giving pixel-wise relevance activations to the input.

At last, we would have loved to have more time to perform more training procedures. One of the things from this work that are pending is to understand why the exponential scheduling is the better performing one. Maybe we could run several runs or extend the exploration time from 1M to several millions. Additionally, we could try several functions from \cite{zhang2022kdecay}, where they propose several decay schedulers for the learning rate of supervised learning set-ups, that we could use to schedule the decay of the exploration rate.