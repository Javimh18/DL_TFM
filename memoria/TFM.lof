\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces (From \cite {markovdecisionprocessgithub}) Markov Decision Process with 5 states as the circles, actions are modeled with the red letters over the arrows, that represent the transition between states. When a transition is performed, a reward is returned (letter R), and a new state is achieved. We can see that there might be transitions where we cannot take an action, and we are subject to the transition probabilities (the dynamics) of the MDP.}}{6}{figure.caption.9}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Aligning inputs using the attention mechanism. First, the inputs $X_1, X_2, X_3, ... X_{\tau }$ are encoded using a bi-directional RNN, obtaining the embeddings $\overrightarrow {h} , \overleftarrow {h}$. This embeddings are then pondered by the product of the learnable parameters $\alpha _{i \tau }$ for each input, creating the context vector $c_t$. Finally, $c_t$ is fed into the decoder, updating the hidden state $s_t$ and producing the output $y_t$.}}{11}{figure.caption.12}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Multi-head attention. The embeddings from Q, K and V are computed and fed into the attention module. Once computed, the attention matrix is computed for each of the heads, and concatenated to then, be forwarded onto a linear layer, that computes the embedding of the attention-weighted input.}}{12}{figure.caption.13}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces In figure \ref {fig:transformer_arch} (left), from \cite {vaswani2023attention}, the complete transformer architecture is presented. In figure \ref {fig:translation_attn} (right) from \cite {translationmin2023attention}, an attention matrix is presented for a sentence and its translated counter-part. The weights for each word quantify how much \textit {attention} is the model paying to the words from the original sentence in order to produce the translation to the new sentence.}}{13}{figure.caption.14}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces In figure \ref {fig:transformer_arch} (left) the ViT transformer architecture is presented. In figure \ref {fig:translation_attn} (right) an attention map is presented. The model that produced the attention maps was trained to track people on a scene, and it is why, the higher attention weights (red) are put in the pixels where people is present.}}{14}{figure.caption.15}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Swin Transformer architecture and its building blocks. The transformer block, figure \ref {fig:swinblocks}, has two phases. In the first one (left-most block), it performs a layer normalization, followed by a window-MSHA block that is forwarded onto a MLP with a neurons ratio of 4. In the second, it makes use of the shifted window MSHA, instead of the window-MHSA. In figure \ref {fig:swinarchitecture} we can see the whole SWIN Transformer architecture, where each of the "stages" are built upon the transformer blocks, that have as input the patch merged embeddings that are passed at each stage.}}{14}{figure.caption.16}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Window shifting in the SWIN Transformer architecture (figure \ref {fig:windowshifting}). From the window partition, shift to the bottom right, so the A, B, C patches are rearranged in the mirrored side of the image. Patch Merging (figure \ref {fig:patchmerging}), picks blocks of 2 by 2 patches inside a window and concatenates them along the channel dimension.}}{15}{figure.caption.17}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces In figure \ref {fig:landing} (left) the goal is to land securely the spaceship (in pruple). In figure \ref {fig:translation_attn} (right) the goal is for the ant to balance itself and start walking.}}{18}{figure.caption.18}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Shape of the proposed exploration functions for the DQN agent}}{44}{figure.caption.37}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Implementation overview of the whole pipeline that we have developed for this work.}}{45}{figure.caption.40}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces TD Returns}}{58}{figure.caption.43}%
\addvspace {10\p@ }
\addvspace {10\p@ }
