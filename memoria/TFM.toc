\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Objectives}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Report structure}{3}{section.1.3}%
\contentsline {chapter}{\numberline {2}Related work}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}An introduction to the RL problem}{5}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Formalizing RL}{5}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Value-based RL}{7}{subsection.2.1.2}%
\contentsline {subsubsection}{Value function approximation}{8}{section*.10}%
\contentsline {subsection}{\numberline {2.1.3}Policy-based RL}{9}{subsection.2.1.3}%
\contentsline {subsubsection}{Policy Gradient Theorem}{10}{section*.11}%
\contentsline {section}{\numberline {2.2}Attention-based models}{10}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}What is attention?}{10}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}The Transformer Architecture}{11}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Vision Transformers}{12}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}SWIN Transformer}{13}{subsection.2.2.4}%
\contentsline {section}{\numberline {2.3}Explainable AI}{15}{section.2.3}%
\contentsline {subsubsection}{Sensitive Analysis}{16}{section*.19}%
\contentsline {subsubsection}{Layer-wise Relevance Propagation}{17}{section*.20}%
\contentsline {subsubsection}{Class Activation Maps}{17}{section*.21}%
\contentsline {section}{\numberline {2.4}Reinforcement Learning using Transformers}{18}{section.2.4}%
\contentsline {section}{\numberline {2.5}Explainable RL using Attention}{19}{section.2.5}%
\contentsline {chapter}{\numberline {3}Design and development}{21}{chapter.3}%
\contentsline {section}{\numberline {3.1}Farama's gymnasium library}{21}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Environment dynamics}{22}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Processing techniques of an environment}{23}{subsection.3.1.2}%
\contentsline {subsubsection}{Skip N-Frames}{23}{section*.23}%
\contentsline {subsubsection}{Gray-Scale Observation}{24}{section*.24}%
\contentsline {subsubsection}{Resize Observation}{25}{section*.25}%
\contentsline {subsubsection}{Additional Wrappers}{25}{section*.26}%
\contentsline {section}{\numberline {3.2}Agents development}{26}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}DQN and DDQN training loop}{26}{subsection.3.2.1}%
\contentsline {subsubsection}{Explaining the algorithm}{26}{section*.27}%
\contentsline {subsubsection}{Main loop implementation}{27}{section*.28}%
\contentsline {subsubsection}{Digging deeper: the DQN agent}{28}{section*.29}%
\contentsline {subsubsection}{DDQN training loop: A minor update}{31}{section*.30}%
\contentsline {subsection}{\numberline {3.2.2}Attention-based models: Vision Transformer}{32}{subsection.3.2.2}%
\contentsline {subsubsection}{Constructor}{32}{section*.31}%
\contentsline {subsubsection}{Patch Embedding}{33}{section*.32}%
\contentsline {subsubsection}{ViT encoder block}{34}{section*.33}%
\contentsline {subsubsection}{ViT Attention Module}{34}{section*.34}%
\contentsline {subsubsection}{The forward method}{35}{section*.35}%
\contentsline {subsection}{\numberline {3.2.3}Attention-based models: SWIN Transformer}{36}{subsection.3.2.3}%
\contentsline {subsubsection}{Constructor}{36}{section*.36}%
\contentsline {subsubsection}{BasicLayer}{37}{section*.37}%
\contentsline {subsubsection}{SwinTransformerBlock}{38}{section*.38}%
\contentsline {subsubsection}{Window Attention}{41}{section*.39}%
\contentsline {subsubsection}{The forward method}{42}{section*.40}%
\contentsline {section}{\numberline {3.3}Setting up the pipeline}{42}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Trainer}{43}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Logger}{43}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Schedulers}{46}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Extracting explainability features}{49}{subsection.3.3.4}%
\contentsline {subsubsection}{Extracting attention maps}{49}{section*.42}%
\contentsline {subsubsection}{Extracting Grad-CAM's activation maps}{50}{section*.45}%
\contentsline {subsection}{\numberline {3.3.5}Additional implementations}{51}{subsection.3.3.5}%
\contentsline {subsubsection}{Configuration files}{51}{section*.47}%
\contentsline {subsubsection}{Checkpoints}{52}{section*.48}%
\contentsline {subsection}{\numberline {3.3.6}Overview}{52}{subsection.3.3.6}%
\contentsline {chapter}{\numberline {4}Evaluation}{53}{chapter.4}%
\contentsline {section}{\numberline {4.1}Experimental set-up}{53}{section.4.1}%
\contentsline {section}{\numberline {4.2}Training results}{54}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Exploration schedules}{55}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Attention models training comparison}{55}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}Explainability features}{55}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Attention maps results}{55}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Activation maps results}{55}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Comparison}{55}{subsection.4.3.3}%
\contentsline {chapter}{\numberline {5}Conclusions and future work}{57}{chapter.5}%
\contentsline {section}{\numberline {5.1}Conclusions}{57}{section.5.1}%
\contentsline {section}{\numberline {5.2}Future work}{57}{section.5.2}%
\contentsline {chapter}{Bibliography}{59}{chapter*.54}%
\contentsline {chapter}{Appendix}{65}{section*.55}%
\contentsline {chapter}{\numberline {A}Reinforcement Learning}{65}{appendix.A}%
\contentsline {section}{\numberline {A.1}Classic learning methods in RL}{65}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Monte Carlo estimation:}{65}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Temporal Difference Learning:}{65}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}On-Policy Control: }{67}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}Off-Policy Control: }{68}{subsection.A.1.4}%
\contentsline {subsection}{\numberline {A.1.5}Actor-Critic Methods}{68}{subsection.A.1.5}%
\contentsline {chapter}{\numberline {B}Attention Mechanism}{69}{appendix.B}%
\contentsline {section}{\numberline {B.1}Understanding the attention mechanism}{69}{section.B.1}%
\contentsline {section}{\numberline {B.2}From attention to self-attention}{70}{section.B.2}%
\contentsline {chapter}{\numberline {C}Schedulers}{73}{appendix.C}%
\contentsline {section}{\numberline {C.1}Gamma for the exponential equation}{73}{section.C.1}%
\contentsline {section}{\numberline {C.2}Gamma for the product of exponential equation}{73}{section.C.2}%
