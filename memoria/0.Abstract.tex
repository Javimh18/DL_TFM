%%%%%%%%%%%%%% ABSTRACT PAGE / RESUMEN (VERSION ESPAÑOL) %%%%%%%%%%
\pagenumbering{roman} \setcounter{page}{1}
%\addcontentsline{toc}{chapter}{\numberline{}{Resumen}} % Uncomment to add in TOC
\chapter*{Resumen}
La explicabilidad en el campo de la Inteligencia Artificial (IA) es una de las tareas más importantes en la actualidad, especialmente en el ámbito del Deep Learning. Las redes neuronales, conocidas por su gran capacidad de aprendizaje, son el estado del arte en casi todas las subdisciplinas de la IA gracias a su habilidad de representación de caracteristicas en espacios de alta dimensionalidad. Pese a esto, este avance tiene un gran problema: la explicabilidad. Recientemente, con la introducción del mecanismo de self-attention (auto-atención) se ha arrojado un haz de luz, ya que trabajos en el campo de la visión artificial han demostrado su utilidad a la hora de entender a qué da mas importancia una red cuando esta genera una salida.

Este trabajo intenta abordar este problema en el paradigma del aprendizaje por refuerzo (reinforcement learning). Uno de los avances más importantes en la última década fue la introducción de redes neuronales profundas para estimar el valor de los estados y acciones, pero a costa de la explicabilidad. Dado que queremos entender cuáles son elementos de un entorno que influyen en las decisiones de un agente, necesitamos una función característica que nos brinde información sobre cómo la red está procesa los datos de entrada. Aprovechando el mecanismo de self-attention, nuestro objetivo es comprobar si existe explicabilidad para el comportamiento de un agente en varios entornos y ver si hay una evidencia general y aplicable a múltiples configuraciones donde las decisiones del modelo puedan explicarse mediante el mecanismo de atención. La explicabilidad es una de las piedras angulares de la IA aplicada, ya que el creciente interés en diferentes sectores en la industria como las finanzas o la automoción puede verse frenado por la falta de explicabilidad en los modelos utilizados.

Primero, introduciremos el problema del aprendizaje por refuerzo, ya que se sale un poco del alcance del programa de master donde se presenta este TFM. Presentaremos los bloques básicos de la estructura de este campo y los descubrimientos relevantes dentro del estado del arte. Después, introduciremos modelos basados en atención, especialmente en el componente "encoder" del modelo transformer, y cómo se adaptaron de su dominio original (procesamiento del lenguaje natural) al dominio de la visión artificial. Luego, explicaremos el desarrollo de estos modelos y junto a esto, el pipeline desarrollado para obtener resultados en diferentes entornos y configuraciones. Una vez que hayamos obtenido los resultados, proporcionaremos un conjunto de evidencias y hallazgos en diferentes situaciones relevantes donde ponemos a prueba nuestra hipótesis del mecanismo de atención como herramienta para la explicabilidad. Finalmente, discutiremos la relevancia de nuestros hallazgos y propondremos algunos contraejemplos donde nuestra hipótesis puede fallar, y propondremos nuevas líneas de investigación sobre este tema.

\vfill
\section*{Palabras clave}
Redes neuronales, Aprendizaje por refuerzo, explicabilidad, atención.

%%%%%%%%%%%%%% ABSTRACT PAGE / RESUMEN (ENGLISH VERSION) %%%%%%%%%%
\newpage
%\addcontentsline{toc}{chapter}{\numberline{}{Abstract}} % Uncomment to add in TOC
\chapter*{Abstract}
Explainability in the field of Artificial Intelligence (AI) is one of the most important tasks nowadays, particularly in the realm of Deep Learning. Neural networks, known for their advanced learning capabilities, are the state-of-the-art systems in almost every sub-discipline of AI due to their capabilities of processing high-dimensional data. However, this advancement comes at a cost: explainability. Recently, the introduction of the self-attention mechanism has changed this, as recent works have shown that it can provide a wider context for understanding what the network is 'looking' at when generating an output.

Our work tries to tackle this problem in the reinforcement learning problem. One of the most substantial advancements in the last decade was the introduction of deep neural networks to estimate the value of states and actions, but at the cost of explainability. Since we want to understand the elements of an environment that influence an agent's decisions, we need a characteristic function that may give us information on how the network is processing the input data. Leveraging the self-attention mechanism, our aim is to find explanations for an agent's behavior in several environments and see if there is general evidence across multiple setups where the model's decisions can be explained by the attention mechanism. Explainability is one of the cornerstones of applied AI, as the growing interest in different sectors such as industrial, finance, or automotive may be halted by the lack of explainability.

First, we will introduce the problem of reinforcement learning, as it falls slightly outside the scope of this master's program. We will introduce the basic blocks of how this field is structured and the relevant findings that represent the state of the art. After that, we will introduce attention-based models, based on the transformer encoder, and how they were adapted from their original domain (natural language processing) to the computer vision domain. Then, we will explain the development of these models along with the corresponding software architecture that made it possible for us to develop the pipeline where we run our experiments. Once we have obtained the results, we will provide a set of evidence and findings in different relevant situations where we test our hypothesis of the attention mechanism as a tool for decision-making explainability. Finally, we will discuss the relevance of our findings and some counter-examples where our hypothesis may be a bit shaky and propose new lines of research on this topic.

\vfill
\section*{Keywords}
Neural networks, reinforcement learning, explainability, attention.

%%%%%%%%%%%%%% ACKS PAGE / AGRADECIMIENTOS %%%%%%%%%%
\newpage
%\addcontentsline{toc}{chapter}{\numberline{}{Acknoledgements}} % Uncomment to add in TOC
\chapter*{Acknowledgements}
I would like to thank my master thesis advisor Luis Lago for his knowledge, his advice and our talks on AI. To my family and friends, for believing in me and give me the courage to always push further. And to the Universidad de Alcalá, especially to the GEINTRA research group for the facilities they have provided me to develop this work.