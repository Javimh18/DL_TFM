%%%%%%%%%%%%%% ABSTRACT PAGE / RESUMEN (VERSION ESPAÑOL) %%%%%%%%%%
\pagenumbering{roman} \setcounter{page}{1}
%\addcontentsline{toc}{chapter}{\numberline{}{Resumen}} % Uncomment to add in TOC
\chapter*{Resumen}
La explicabilidad en el campo de la Inteligencia Artificial (IA) es una de las tareas más importantes en la actualidad, especialmente en el ámbito del Deep Learning. Las redes neuronales, conocidas por su gran capacidad de aprendizaje, son el estado del arte en casi todas las subdisciplinas de la IA gracias a su habilidad de representación de caracteristicas en espacios de alta dimensionalidad. Pese a esto, este avance tiene un gran problema: la explicabilidad. Recientemente, con la introducción del mecanismo de self-attention (auto-atención) se ha arrojado un haz de luz, ya que trabajos en el campo de la visión artificial han demostrado su utilidad a la hora de entender a qué da mas importancia una red cuando esta genera una salida.

Este trabajo intenta abordar este problema en el paradigma del aprendizaje por refuerzo (reinforcement learning). Uno de los avances más importantes en la última década fue la introducción de redes neuronales profundas para estimar el valor de los estados y acciones, pero a costa de la explicabilidad. Dado que queremos entender cuáles son elementos de un entorno que influyen en las decisiones de un agente, necesitamos una función característica que nos brinde información sobre cómo la red está procesa los datos de entrada. Aprovechando el mecanismo de self-attention, nuestro objetivo es comprobar si existe explicabilidad para el comportamiento de un agente en varios entornos y ver si hay una evidencia general y aplicable a múltiples configuraciones donde las decisiones del modelo puedan explicarse mediante el mecanismo de atención. La explicabilidad es una de las piedras angulares de la IA aplicada, ya que el creciente interés en diferentes sectores en la industria como las finanzas o la automoción puede verse frenado por la falta de explicabilidad en los modelos utilizados.

En este informe, reflejamos las diferentes pruebas y configuraciones de aprendizaje por refuerzo basadas en Deep Q-networks para modelos basados en atención. Hemos entrenado agentes para jugar dos juegos clásicos del entorno Atari 2600. También proponemos varias planificaciones de exploración para el modelo, comparándolos y analizando el por qué de que algunos funcionan mejor que otros. Para explicar el comportamiento del modelo, hemos extraído evidencias visuales para comprender e interpretar las decisiones del modelo, con algunos hallazgos interesantes sobre los mapas de atención y los mapas de activación utilizando algoritmos bien establecidos que se adaptan a este propósito. Argumentamos que hay espacio para entender las decisiones del agente cuando realiza acciones en un entorno, y creemos que la aplicación de este tipo de técnicas a problemas más complejos podría ofrecer una mejor perspectiva sobre por qué los agentes se comportan como lo hacen. Además, hemos desarrollado un pipeine que ha facilitado muchas configuraciones para realizar nuestros experimentos, adaptándose a la naturaleza de prueba y error del paradigma de Machine Learning.

\vfill
\section*{Palabras clave}
Redes neuronales, Aprendizaje por refuerzo, explicabilidad, atención.

%%%%%%%%%%%%%% ABSTRACT PAGE / RESUMEN (ENGLISH VERSION) %%%%%%%%%%
\newpage
%\addcontentsline{toc}{chapter}{\numberline{}{Abstract}} % Uncomment to add in TOC
\chapter*{Abstract}
Explainability in the field of Artificial Intelligence (AI) is one of the most important tasks nowadays, particularly in the realm of Deep Learning. Neural networks, known for their advanced learning capabilities, are the state-of-the-art systems in almost every sub-discipline of AI due to their capabilities of processing high-dimensional data. However, this advancement comes at a cost: explainability. Recently, the introduction of the self-attention mechanism has changed this, as recent works have shown that it can provide a wider context for understanding what the network is 'looking' at when generating an output.

Our work tries to tackle this problem in the reinforcement learning problem. One of the most substantial advancements in the last decade was the introduction of deep neural networks to estimate the value of states and actions. Since we want to understand the elements of an environment that influence an agent's decisions, we need a characteristic function that may give us information on how the network is processing the input data. Taking advantage of the self-attention mechanism, our aim is to find explanations for an agent's behavior in several environments and see if there is general evidence across multiple setups where the model's decisions can be explained by the attention mechanism. Explainability is one of the most important fields of applied AI, as the growing interest in different sectors such as industrial, finance, or automotive may be halted by the lack of explainability.

For this report, we have tried several reinforcement learning set-ups based in Deep Q-networks. We have trained agents based on attention models to play two classic Atari 2600 games. We also propose several exploration schedules for the model, comparing them and analysing why some may work better than others. For explaining the model's behaviour, we have extracted visual evidences to explain and interpret the model's decisions, with some interesting findings about the attention maps and the activation maps using well-established algorithms that suit this purpose. We argue that there is room for understanding the agent decisions when it perform actions in an environment, and we believe that the applications of this type of techniques to more complex problems could give a better view on why agents behave as they do. Additionally, we have developed a pipeline that facilitated lots of experimental set-ups, suiting the trial-error nature of the Machine Learning paradigm.

\vfill
\section*{Keywords}
Neural networks, reinforcement learning, explainability, attention.

%%%%%%%%%%%%%% ACKS PAGE / AGRADECIMIENTOS %%%%%%%%%%
\newpage
%\addcontentsline{toc}{chapter}{\numberline{}{Acknoledgements}} % Uncomment to add in TOC
\chapter*{Acknowledgements}
I would like to thank my master thesis advisor Luis Lago for his knowledge, his advice and our talks on AI. To my family and friends, for believing in me and give me the courage to always push further. And to the Universidad de Alcalá, especially to the GEINTRA research group for the facilities they have provided me to develop this work.