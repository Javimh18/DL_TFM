\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{intro_rl}
\citation{markovdecisionprocessgithub}
\citation{markovdecisionprocessgithub}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related work}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:related-work}{{2}{5}{Related work}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}An introduction to the RL problem}{5}{section.2.1}\protected@file@percent }
\newlabel{sec:intro-rl}{{2.1}{5}{An introduction to the RL problem}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Formalizing RL}{5}{subsection.2.1.1}\protected@file@percent }
\newlabel{sec:form-intro-rl}{{2.1.1}{5}{Formalizing RL}{subsection.2.1.1}{}}
\@writefile{brf}{\backcite{intro_rl}{{5}{2.1.1}{subsection.2.1.1}}}
\newlabel{eq:mdp_trajectory}{{2.1}{5}{Formalizing RL}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  (From \cite  {markovdecisionprocessgithub}) Markov Decision Process with 5 states as the circles, actions are modeled with the red letters over the arrows, that represent the transition between states. When a transition is performed, a reward is returned (letter R), and a new state is achieved. We can see that there might be transitions where we cannot take an action, and we are subject to the transition probabilities (the dynamics) of the MDP.}}{6}{figure.caption.9}\protected@file@percent }
\@writefile{brf}{\backcite{markovdecisionprocessgithub}{{6}{2.1}{figure.caption.9}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mdp}{{2.1}{6}{(From \cite {markovdecisionprocessgithub}) Markov Decision Process with 5 states as the circles, actions are modeled with the red letters over the arrows, that represent the transition between states. When a transition is performed, a reward is returned (letter R), and a new state is achieved. We can see that there might be transitions where we cannot take an action, and we are subject to the transition probabilities (the dynamics) of the MDP}{figure.caption.9}{}}
\newlabel{past_ind_fut}{{2.2}{6}{Formalizing RL}{equation.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Value-based RL}{7}{subsection.2.1.2}\protected@file@percent }
\newlabel{sec:val-based-rl}{{2.1.2}{7}{Value-based RL}{subsection.2.1.2}{}}
\newlabel{eq:state_value}{{2.3}{7}{Value-based RL}{equation.2.1.3}{}}
\newlabel{eq:action_value}{{2.4}{7}{Value-based RL}{equation.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Value function approximation}{8}{section*.10}\protected@file@percent }
\newlabel{sec:val-fun_approx}{{2.1.2}{8}{Value function approximation}{section*.10}{}}
\newlabel{eq:value_aprox}{{2.5}{8}{Value function approximation}{equation.2.1.5}{}}
\newlabel{eq:q_value_aprox}{{2.6}{8}{Value function approximation}{equation.2.1.6}{}}
\newlabel{eq:comb_qval}{{2.7}{8}{Value function approximation}{equation.2.1.7}{}}
\newlabel{eq:sgd_objective_function}{{2.8}{8}{Value function approximation}{equation.2.1.8}{}}
\newlabel{eq:sgd_opt}{{2.9}{8}{Value function approximation}{equation.2.1.9}{}}
\newlabel{eq:value_approx_td0}{{2.10}{8}{Value function approximation}{equation.2.1.10}{}}
\citation{mnih2013playing}
\citation{vanhasselt2015deep}
\newlabel{eq:value_approx_tdlambda}{{2.11}{9}{Value function approximation}{equation.2.1.11}{}}
\@writefile{brf}{\backcite{mnih2013playing}{{9}{2.1.2}{equation.2.1.11}}}
\newlabel{eq:dqn_loss}{{2.12}{9}{Value function approximation}{equation.2.1.12}{}}
\@writefile{brf}{\backcite{vanhasselt2015deep}{{9}{2.1.2}{equation.2.1.12}}}
\newlabel{eq:ddqn_loss}{{2.13}{9}{Value function approximation}{equation.2.1.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Policy-based RL}{9}{subsection.2.1.3}\protected@file@percent }
\newlabel{sec:pol-grad-ac}{{2.1.3}{9}{Policy-based RL}{subsection.2.1.3}{}}
\citation{bahdanau2016neural}
\citation{understandingattentionmechanism}
\citation{vaswani2023attention}
\@writefile{toc}{\contentsline {subsubsection}{Policy Gradient Theorem}{10}{section*.11}\protected@file@percent }
\newlabel{sec:pol-grad-theorem}{{2.1.3}{10}{Policy Gradient Theorem}{section*.11}{}}
\newlabel{eq:param_policy}{{2.14}{10}{Policy Gradient Theorem}{equation.2.1.14}{}}
\newlabel{eq:policy_gradient_theorem}{{2.15}{10}{Policy Gradient Theorem}{equation.2.1.15}{}}
\newlabel{eq:reinforce}{{2.16}{10}{Policy Gradient Theorem}{equation.2.1.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Attention-based models}{10}{section.2.2}\protected@file@percent }
\newlabel{sec:attention}{{2.2}{10}{Attention-based models}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}What is attention?}{10}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:whats-attention}{{2.2.1}{10}{What is attention?}{subsection.2.2.1}{}}
\@writefile{brf}{\backcite{bahdanau2016neural}{{10}{2.2.1}{subsection.2.2.1}}}
\@writefile{brf}{\backcite{understandingattentionmechanism}{{10}{2.2.1}{subsection.2.2.1}}}
\citation{model_emsembling_survey}
\citation{vaswani2023attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Aligning inputs using the attention mechanism. First, the inputs $X_1, X_2, X_3, ... X_{\tau }$ are encoded using a bi-directional RNN, obtaining the embeddings $\overrightarrow  {h} , \overleftarrow  {h}$. This embeddings are then pondered by the product of the learnable parameters $\alpha _{i \tau }$ for each input, creating the context vector $c_t$. Finally, $c_t$ is fed into the decoder, updating the hidden state $s_t$ and producing the output $y_t$.}}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig:alignmentattention}{{2.2}{11}{Aligning inputs using the attention mechanism. First, the inputs $X_1, X_2, X_3, ... X_{\tau }$ are encoded using a bi-directional RNN, obtaining the embeddings $\overrightarrow {h} , \overleftarrow {h}$. This embeddings are then pondered by the product of the learnable parameters $\alpha _{i \tau }$ for each input, creating the context vector $c_t$. Finally, $c_t$ is fed into the decoder, updating the hidden state $s_t$ and producing the output $y_t$}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The Transformer Architecture}{11}{subsection.2.2.2}\protected@file@percent }
\newlabel{sec:transformer}{{2.2.2}{11}{The Transformer Architecture}{subsection.2.2.2}{}}
\@writefile{brf}{\backcite{vaswani2023attention}{{11}{2.2.2}{subsection.2.2.2}}}
\newlabel{eq:attn_eq}{{2.17}{11}{The Transformer Architecture}{equation.2.2.17}{}}
\citation{vaswani2023attention}
\citation{translationmin2023attention}
\citation{vaswani2023attention}
\citation{translationmin2023attention}
\citation{vit}
\@writefile{brf}{\backcite{model_emsembling_survey}{{12}{2.2.2}{equation.2.2.17}}}
\@writefile{brf}{\backcite{vaswani2023attention}{{12}{2.2.2}{equation.2.2.17}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Multi-head attention. The embeddings from Q, K and V are computed and fed into the attention module. Once computed, the attention matrix is computed for each of the heads, and concatenated to then, be forwarded onto a linear layer, that computes the embedding of the attention-weighted input.}}{12}{figure.caption.13}\protected@file@percent }
\newlabel{fig:multihead_attn}{{2.3}{12}{Multi-head attention. The embeddings from Q, K and V are computed and fed into the attention module. Once computed, the attention matrix is computed for each of the heads, and concatenated to then, be forwarded onto a linear layer, that computes the embedding of the attention-weighted input}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Vision Transformers}{12}{subsection.2.2.3}\protected@file@percent }
\newlabel{sec:vis-transformers}{{2.2.3}{12}{Vision Transformers}{subsection.2.2.3}{}}
\@writefile{brf}{\backcite{vit}{{12}{2.2.3}{subsection.2.2.3}}}
\citation{liu2021swin}
\newlabel{fig:transformer_arch}{{2.4a}{13}{Transformer architecture.\caption@thelabel }{figure.caption.14}{}}
\newlabel{sub@fig:transformer_arch}{{a}{13}{Transformer architecture.\caption@thelabel }{figure.caption.14}{}}
\newlabel{fig:translation_attn}{{2.4b}{13}{Cross-Attention weights for the translation task\caption@thelabel }{figure.caption.14}{}}
\newlabel{sub@fig:translation_attn}{{b}{13}{Cross-Attention weights for the translation task\caption@thelabel }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces In figure \ref {fig:transformer_arch} (left), from \cite  {vaswani2023attention}, the complete transformer architecture is presented. In figure \ref {fig:translation_attn} (right) from \cite  {translationmin2023attention}, an attention matrix is presented for a sentence and its translated counter-part. The weights for each word quantify how much \textit  {attention} is the model paying to the words from the original sentence in order to produce the translation to the new sentence.}}{13}{figure.caption.14}\protected@file@percent }
\@writefile{brf}{\backcite{vaswani2023attention}{{13}{2.4}{figure.caption.14}}}
\@writefile{brf}{\backcite{translationmin2023attention}{{13}{2.4}{figure.caption.14}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}SWIN Transformer}{13}{subsection.2.2.4}\protected@file@percent }
\newlabel{sec:swin-transformer}{{2.2.4}{13}{SWIN Transformer}{subsection.2.2.4}{}}
\@writefile{brf}{\backcite{liu2021swin}{{13}{2.2.4}{subsection.2.2.4}}}
\newlabel{fig:vit_arch}{{2.5a}{14}{Transformer architecture.\caption@thelabel }{figure.caption.15}{}}
\newlabel{sub@fig:vit_arch}{{a}{14}{Transformer architecture.\caption@thelabel }{figure.caption.15}{}}
\newlabel{fig:attn_maps}{{2.5b}{14}{Attention weights for human detection task\caption@thelabel }{figure.caption.15}{}}
\newlabel{sub@fig:attn_maps}{{b}{14}{Attention weights for human detection task\caption@thelabel }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces In figure \ref {fig:transformer_arch} (left) the ViT transformer architecture is presented. In figure \ref {fig:translation_attn} (right) an attention map is presented. The model that produced the attention maps was trained to track people on a scene, and it is why, the higher attention weights (red) are put in the pixels where people is present.}}{14}{figure.caption.15}\protected@file@percent }
\newlabel{fig:swinblocks}{{2.6a}{14}{Swin Transformer block}{figure.caption.16}{}}
\newlabel{sub@fig:swinblocks}{{a}{14}{Swin Transformer block}{figure.caption.16}{}}
\newlabel{fig:swinarchitecture}{{2.6b}{14}{SWIN Transformer architecture}{figure.caption.16}{}}
\newlabel{sub@fig:swinarchitecture}{{b}{14}{SWIN Transformer architecture}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Swin Transformer architecture and its building blocks. The transformer block, figure \ref {fig:swinblocks}, has two phases. In the first one (left-most block), it performs a layer normalization, followed by a window-MSHA block that is forwarded onto a MLP with a neurons ratio of 4. In the second, it makes use of the shifted window MSHA, instead of the window-MHSA. In figure \ref {fig:swinarchitecture} we can see the whole SWIN Transformer architecture, where each of the "stages" are built upon the transformer blocks, that have as input the patch merged embeddings that are passed at each stage.}}{14}{figure.caption.16}\protected@file@percent }
\newlabel{fig:swin_transformer_parts}{{2.6}{14}{Swin Transformer architecture and its building blocks. The transformer block, figure \ref {fig:swinblocks}, has two phases. In the first one (left-most block), it performs a layer normalization, followed by a window-MSHA block that is forwarded onto a MLP with a neurons ratio of 4. In the second, it makes use of the shifted window MSHA, instead of the window-MHSA. In figure \ref {fig:swinarchitecture} we can see the whole SWIN Transformer architecture, where each of the "stages" are built upon the transformer blocks, that have as input the patch merged embeddings that are passed at each stage}{figure.caption.16}{}}
\citation{swin_unet}
\citation{swin_unet}
\citation{rl_transformers2023survey}
\citation{vit_q_learning_sample_eff}
\newlabel{fig:windowshifting}{{2.7a}{15}{Window shifting and cyclic shifting module}{figure.caption.17}{}}
\newlabel{sub@fig:windowshifting}{{a}{15}{Window shifting and cyclic shifting module}{figure.caption.17}{}}
\@writefile{brf}{\backcite{swin_unet}{{15}{2.7b}{figure.caption.17}}}
\newlabel{fig:patchmerging}{{2.7b}{15}{Patch merging module for hierarchical feature aggregation (image from\cite {swin_unet})}{figure.caption.17}{}}
\newlabel{sub@fig:patchmerging}{{b}{15}{Patch merging module for hierarchical feature aggregation (image from\cite {swin_unet})}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Window shifting in the SWIN Transformer architecture (figure \ref {fig:windowshifting}). From the window partition, shift to the bottom right, so the A, B, C patches are rearranged in the mirrored side of the image. Patch Merging (figure \ref {fig:patchmerging}), picks blocks of 2 by 2 patches inside a window and concatenates them along the channel dimension.}}{15}{figure.caption.17}\protected@file@percent }
\newlabel{fig:combined}{{2.7}{15}{Window shifting in the SWIN Transformer architecture (figure \ref {fig:windowshifting}). From the window partition, shift to the bottom right, so the A, B, C patches are rearranged in the mirrored side of the image. Patch Merging (figure \ref {fig:patchmerging}), picks blocks of 2 by 2 patches inside a window and concatenates them along the channel dimension}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning using Transformers}{15}{section.2.3}\protected@file@percent }
\newlabel{sec:rl-with-attention}{{2.3}{15}{Reinforcement Learning using Transformers}{section.2.3}{}}
\@writefile{brf}{\backcite{rl_transformers2023survey}{{15}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{vit_q_learning_sample_eff}{{15}{2.3}{section.2.3}}}
\citation{meng2024deep}
\citation{vanhasselt2015deep}
\citation{Bellemare_2013}
\citation{janner2021offline}
\citation{chen2021decision}
\citation{wang2022bootstrapped}
\citation{BRAMLAGE202210}
\citation{LEONG2017451}
\citation{BRAMLAGE202210}
\@writefile{brf}{\backcite{meng2024deep}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{vanhasselt2015deep}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{Bellemare_2013}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{janner2021offline}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{chen2021decision}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{wang2022bootstrapped}{{16}{2.3}{section.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Explainable RL using Attention}{16}{section.2.4}\protected@file@percent }
\@writefile{brf}{\backcite{BRAMLAGE202210}{{16}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{LEONG2017451}{{16}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{BRAMLAGE202210}{{16}{2.4}{section.2.4}}}
\@setckpt{2.RelatedWork}{
\setcounter{page}{17}
\setcounter{equation}{17}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{section@level}{1}
\setcounter{Item}{2}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{16}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{caption@flags}{6}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{2}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstnumber}{1}
\setcounter{lstlisting}{0}
}
