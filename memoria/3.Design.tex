%%%%%%%%%%%% DISEÃ‘O Y DESARROLLO / DESIGN & IMPLEMENTATION CHAPTER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Design and development}
\label{cha:design-development}
As previously explained, one of the main differences in the RL paradigm is how the agent acquires data. Opposite to the datasets or databases in the supervised learning environment, where we have an input and a ground truth structured in such a way that the model can learn from the data adjusting itself to the ground truth, here, the data is obtained via observations from an environment, that are associated with a reward and a observation of the next state. On the other hand, we have to create a seamless pipeline for a deep neural network model that interacts with this environment and learns from it using two different off-policy algorithms: DQN and DDQN.

In this section we will explain step by step how we implemented this framework, from the environment instantiation, the post-processing of the observations provided by the environment, how we modelled the interactions of the agent over the environment and which tools we used for the memory replay module of the several agents that we developed. Given the short amount for experiments that we had, we have only trained agents for two environments for the Arcade Learning environment: \textbf{MsPacman-v5} and \textbf{Demon-Attack-v5}. Additionally, we will also explain how we fitted deep learning into this problem, going from less complex models as CNNs to more complex such as transformers. 

\section{Farama's gymnasium library}
The Farama's gymnasium (Figure \ref{fig:faramas-gym}) library was originally developed and released by OpenAI and was a huge leap in the RL realm, since before this, researchers had to develop their own environments or games to test their agents. Using games is a great way to test if an agent is actually learning from an environment, and one of the first frameworks that showed this was the Arcade Learning Environment or ALE \cite{Bellemare_2013}, where Bellemare \textit{et al.} proposed a platform with different games that were originally developed for the Atari 2600 console. ALE was quite impactful, since lots of relevant works have benefited from this platform to test and launch their agents, providing a kind of benchmark. The ALE environment was included in the Farama's gym, along with other platforms such as Multi-Joint dynamics with Contact (MuJoCo), a physics engine that test an agents ability to perform control over complex robotic entities (Figure \ref{fig:ant}) or Box2D, a platform that involve toy games in order to perform control on several vehicles, such as space-ships or cars (Figure \ref{fig:landing}).

This library will be our starting point. A deep understanding in how it works is crucial for an efficient and insightful development of our deep learning-based agents.

\begin{figure}[!h] 
	\centering
	\subfloat[Gym landing environment.]{%
		\includegraphics[width=0.45\linewidth]{figures/gym_landing.png}%
		\label{fig:landing}%
	}%
	\hfill%
	\subfloat[Ant MuJoCo environment]{%
		\includegraphics[width=0.45\linewidth]{figures/gym_ant_mujoco.png}%
		\label{fig:ant}%
	}%
	\caption{In figure \ref{fig:landing} (left) the goal is to land securely the spaceship (in purple). In figure \ref{fig:ant} (right) the goal is for the ant to balance itself and start walking.}
	\label{fig:faramas-gym}
\end{figure}

\subsection{Environment dynamics}
\label{sec:env_dyn}

The way that Farama's gym models agent-environment interaction is quite similar to what we explained in section \ref{sec:form-intro-rl}. In the listing \ref{code:init_env} we see a basic example of how an interaction with an environment can be performed. First, we need to instantiate the environment, calling the \inlinecode{gym.make} function, with the argument passed as the name of the environment we are going to interact with. In order to initialize the environment, we must reset it, using the \inlinecode{env.reset()} method. Once we have our environment ready, we can proceed to interact with it performing actions. The number of actions, called action space, varies depending on the environment, but the gymnasium library offers methods and attributes that facilitate them in run-time, such as \inlinecode{env.action_space.n}. In the example provided, the action taken is random, using \inlinecode{env.action_space.sample()}, but we can use more complex methods to select the action our agent is going to perform (i.e. a neural network). 

\begin{lstlisting}[caption={Initialization of an environment in Farama's gymnasium}, label={code:init_env}]
import gymnasium as gym
env = gym.make("LunarLander-v2", render_mode="human")
observation, info = env.reset(seed=42)
for _ in range(1000):
	action = env.action_space.sample() 
	observation, reward, terminated, truncated, info = env.step(action)
	
	if terminated or truncated:
		observation, info = env.reset()

env.close()
\end{lstlisting}

In the RL framework, when an action is selected, the agent performs said action in the environment, and obtains a reward and a next observation. By repeatedly doing so, a series of trajectories are created (as defined in equation \ref{eq:mdp_trajectory}), until the agent reaches a final step. The whole trajectory it is called an episode. In the code, the method \inlinecode{env.step()} is in charge of performing the steps of an episode, simulating in each time-step what the outcome will be, returning the following:

\begin{itemize}
	\item \textbf{Observation}: What the environment lets the agent see about its state. If $\text{state}=\text{observation}$ then the environment is a MDP, if not, then its a POMDP.
	\item  \textbf{Reward}: The numerical signal that represents how good (or bad) the actions the agent is taking are.
	\item \textbf{Terminated}: Boolean flag that represents if the episode has ended.
	\item  \textbf{Truncated}: Boolean flag that represents if there has been an unexpected situation that resulted in the current episode ending prematurely. 
	\item \textbf{Info}: Additional information from the current state of the environment, such as current lives if the game is a survival, the current score or even and additional final observation if the state is terminal.
\end{itemize}

The interacting loop could go forever if no additional condition is stated, that is why using the truncated and terminated flags is the common approach to reset the environment and start a new episode. Once the simulation is over, we can close the environment using the \inlinecode{env.close()} method.

\subsection{Processing techniques of an environment}
\label{sec:proc_env}

In most of the environments, observations are usually RGB frames displaying the current situation of the environment. Since our models deal with visual data, it is quite convenient to use some pre-processing techniques that simplify the observation into something that is more "consumable" by the DQN agent. Farama's gymnasium provides an API that allows modifications of the environment, called \textbf{wrappers}. These wrappers are conceived so that they can alter the original behaviour of the environment, in terms of the reward, number of frames skipped between steps or the colour gamut of the observation. This operations are analogous to the $\phi$ operator defined in equation \ref{eq:comb_qval}, since wrappers try to extract the most essential features from the raw input.

In our implementation, we make use of four custom wrappers, along with some pre-defined offered by the gymnasium library. This will ease the observation processing a make lightweight computations by discarding redundant information provided by the environment. In the following section we explain some of the custom wrappers that we have developed, along with some pre-defined ones.

\subsubsection{Skip N-Frames}
\label{sec:skip_wrapper}
In a RL environment, the variation between two consecutive frames usually is very little. If we pass each observation the environment provides, we end up with lots of redundant information which means that lots of computational resources are used in vane. One of the main approaches that is done in RL and other fields such as robotics or autonomous driving, where high frequency sensors are used is to skip a fixed amount of frames per observation. This technique is usually known as "frame skipping". The code developed for this wrapper is shown in listing \ref{code:skip_wrapper}. One thing to note is that in terms of reward, we add in the n-th frame the accumulated rewards from the skipped observations, since we are comprising the information of n-frames in just one observation.

\begin{lstlisting}[caption={Frame skip wrapper}, label={code:skip_wrapper}]
class SkipFrame(gym.Wrapper):
	def __init__(self, env: gym.Env, skip):
	"""Returns only the `skip`-th frame."""
		super().__init__(env)
		self._skip = skip
	
	def step(self, action):
	"""Repeat action, and sum reward"""
		total_reward = 0.0
		for _ in range(self._skip):
			obs, reward, done, trunk, info = self.env.step(action)
			total_reward += reward
			if done or trunk:
				break
	
	return obs, total_reward, done, trunk, info

\end{lstlisting}

\subsubsection{Gray-Scale Observation}
Most of the gymnasium's environments are in RGB colour coding. This may be useful for us humans, since we can extract additional information about the state of the game (i.e. detect if the ghosts in the Pac-man game are or colour blue or not, which gives us a hint on wether we can "eat" them and win points). Machine vision does not work like this, and this information sometimes is seen as redundant, since for a machine, the greyscale palette is enough. Also, it has its limitations in terms of computing power and resources. RGB images have three channels, which makes the observations more expensive to process, specially if we are stacking n-frames to them. Also, since we are dealing with experience replay models, storing these observations along with the collected experience can result in more memory consumption, which is not optimal in the training process. To solve this, the RGB observations are transformed to integer grey-scale. In this case, we are going to make use of PyTorch's transforms from the torchvision library that adds several visual transformations such as cropping, rotation, inversions or colour palette transformations. Using these transformations, we can also cast the original data-type of the observation (\inlinecode{numpy.array}) to a \inlinecode{torch.Tensor} datatype, which is convenient for processing said observation using a neural network using PyTorch framework. The code developed to apply this transformation to the environment is displayed in listing \ref{code:grayscale_wrapper}.

\begin{lstlisting}[caption={Grayscale frame wrapper}, label={code:grayscale_wrapper}]
class GrayScaleObservation(gym.ObservationWrapper):
	def __init__(self, env: gym.Env):
		super().__init__(env)
		obs_shape = self.observation_space.shape[:2]
		# since we are wrapping the observation that an environment provides,
		# we must update the observation space to match the new wrapped env.
		self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)
	
	def permute_orientation(self, observation):
		# permute [H, W, C] array to [C, H, W] tensor for pytorch model
		observation = np.transpose(observation, (2, 0, 1))
		observation = torch.tensor(observation.copy(), dtype=torch.float)
		return observation
	
	def observation(self, observation):
		observation = self.permute_orientation(observation)
		# since we have updated the observation space attribute, we must 
		# do so with the pixels from the env, casting them from RGB to grayscale
		transform = T.Grayscale()
		observation = transform(observation)
		return observation
	
\end{lstlisting}

\subsubsection{Resize Observation}
The size of an image is a crucial element in neural networks for visual data processing, a larger image is preferable, but it also comes at a cost, specially in the training stage. High resolution images require deeper and bigger models to capture all the spatial features, which results in more parameters to adjust during training. The issue comes with the availability of computational resources, since we do not have a cluster with multiple GPUs at our disposal. To adapt to our computational resources, we have added an additional custom wrapper to our environment that resizes the observation from its original shape, $256 \times 240$ to a down-scaled shape of $84 \times 84$. The code for doing so is portrayed in listing \ref{code:resize_wrapper}, where we use torchvision's transforms for doing so. Additionally, we normalize the pixel values by using the \inlinecode{T.Normalize(0,255)} transform.

\begin{lstlisting}[caption={Resize frame wrapper}, label={code:resize_wrapper}]
class ResizeObservation(gym.ObservationWrapper):
	def __init__(self, env: gym.Env, shape):
		super().__init__(env)
		# If shape is just a number it creates a tuple with that same number (i.e. 84 -> (84,84))
		# if is a list creates a "tuple" object with the given shape
		if isinstance(shape, int):
		self.shape = (shape, shape)
		else:
		self.shape = tuple(shape)
		
		obs_shape = self.shape # + self.observation_space.shape[:2]
		self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)
	
	def observation(self, observation):
		# Use torch transforms to resize the observation 
		# to the wanted resize resolution
		transforms = T.Compose(
			[T.Resize(self.shape, antialias=True),
			T.Normalize(0,255)]
		)
		
	# apply the transformation
	observation = transforms(observation).squeeze(0)
	return observation
	
\end{lstlisting}

\subsubsection{Additional Wrappers}
Additionally, there are some libraries such as Stable Baselines 3 \cite{stable-baselines3} or TorchRL \cite{bou2023torchrl} that provide methods and classes that encapsulate lots of the functionality provided by wrappers which were useful for our experiments:

\begin{itemize}
	\item \textbf{ClipRewardEnv}: Maps the original reward signal from the environment to the range [-1, 1]. This has multiple benefits, such as preventing larger gradients that may affect learning due to outliers in the reward distribution or encourage exploration, since the algorithm will not always go for high-reward actions, since all of them are within a small range. This is further explored in works such as \cite{Eschmann2021} where they remark the importance of designing an optimal reward signal for the agent to boost its behaviour.
	\item \textbf{EpisodicLifeEnv}: For games where the agent has several lives, treats losing a life as the end of the episode. This was originally implemented by Silver \textit{et al.} in \cite{mnih2013playing} to help the agent with value estimation.
	\item \textbf{MaxAndSkipEnv}: Frame skipping with some truncated behaviour (similar to what we have already explained in section \ref{sec:skip_wrapper}).
	\item \textbf{NoopResetEnv}: Once the environment is reset, during a certain amount of steps, the agent performs no actions (\textbf{no-op}erations) over the environment.
\end{itemize}

 While it may be true that RL is not the field in AI that receives the most attention, little by little, lots of frameworks and libraries that make quite easy train a model are being launched. Both TorchRL and Stable Baselines are quite useful libraries, and since they are PyTorch oriented, they helped us quite a lot in the development of this work.

\section{Agents development}
\label{sec:agents_dev}

\subsection{DQN  and DDQN training loop}
\label{sec:dqn_training_loop}
\subsubsection{Explaining the DQN algorithm}
Once we defined all the pre-processing techniques for interacting with the environments, we proceeded to develop the DQN algorithm as Silver \textit{et al.} defined in \cite{mnih2013playing}. In this section we will first explain the algorithm and the we will briefly explain how we program it using pure python and PyTorch.

\begin{algorithm}
	\caption{DQN with Experience Replay}
	\label{alg:dqn}
	\begin{algorithmic}[1]
		\State Initialize replay memory $D$ to capacity $N$
		\State Initialize action-value function $Q$ with random weights
		\State Initialize target action-value function $\hat{Q}$ with weights $\theta^{-} = \theta$
		\For{episode = 1, $M$}
		\State Initialize sequence $s_1 = \{x_1\}$ and pre-processed sequence $\phi_1 = \phi(s_1)$
		\For{t = 1, $T$}
		\State With probability $\epsilon$ select a random action $a_t$
		\State otherwise select $a_t = \arg\max_a Q(\phi(s_t), a; \theta)$
		\State Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$
		\State Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$
		\State Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in $D$
		\State Sample random mini-batch of transitions $(\phi_j, a_j, r_j, \phi_{j+1}) \sim D$
		\State Set $y_j = 
		\begin{cases} 
			r_j & \text{if episode terminates at step } j+1 \\
			r_j + \gamma \max_{a'} \hat{Q}(\phi_{j+1}, a'; \theta^{-}) & \text{otherwise}
		\end{cases}$
		\State Perform a gradient descent step on $(y_j - Q(\phi_j, a_j; \theta))^2$ with respect to the network parameters $\theta$
		\State Every $C$ steps reset $\hat{Q} = Q$
		\EndFor
		\EndFor
	\end{algorithmic}
	
\end{algorithm}

The DQN pseudocode is defined in Algorithm \ref{alg:dqn}, where first we define a replay memory. This replay memory is basically a data structure that focus on storing experiences that the agent performs in the environment. Formally, we define as experience the tuple as $(\phi_t, a_t, r_t, \phi_{t+1})$, although when developing the code, we made some minor adjustments for better performance and extended functionality, such as store if the experience resulted in a terminal state. Also, we initialize our action-value function approximator $Q$ with random weights, which we will call the online approximator. From there, we will create our target network (the look-ahead), that will be defined as $\hat{Q}$ and copy the weights $\theta$ from $Q$ (the online/estimation network) to $\hat{Q}$. This ensures that the online function approximator and the target function approximator start the training process under equal circumstances, avoiding inaccurate estimation for the state-action values.

Once these initialization steps are done, we begin the training loop, where we iterate for a fixed number of episodes $N$ in the training process. Since we want to extract the most relevant features, we assume that there is a function $\phi$ that will pre-process the environment, extracting the most relevant features from our input $s_t$ creating the feature vector $\phi(s_t)$. For us, this would be the wrappers explained in section \ref{sec:proc_env}. Then, the episode begin and, for each time step t, up until the episode ends (the agent finds itself in a terminal state) the agent starts playing and learning.

Both DQN and DDQN follow an $\epsilon$-greedy policy as the behavioural policy $\pi$, meaning that with a probability $\epsilon$ a random action will be selected instead of the one most rewarding one, according to our $Q$-value estimation. This encourage exploration, and not falling into a sub-optimal set of $Q$-values as explained in \cite{exploitation_vs_exploration}, where the exploration vs. exploitation dilemma is discussed. For our implementation, we will have and initial exploration rate $\epsilon_0$ and a final exploration rate $\epsilon_f$ and we designed a scheduling that decays the value of $\epsilon$. Doing so provides an exploration stage at first, where the agent is forced to perform lots of random actions, experimenting and evaluating different situations, thus, increasing the variability of the states that it can learn from.

Once the action is performed, the environment will react to it, providing a reward for such action and a new raw observation $x_{t+1}$. This will create a transition tuple that will be stored in the replay memory buffer $D$. 

As DQN is an off-policy algorithm, we will learn by performing actions that do not follow the behavioural policy $\pi$ but the target policy $\mu$. By doing so, we will sample a transition tuple from the replay memory $D$, and applying the greedy policy, $\mu$ obtaining the $Q$-values obtained as the target $y_j$ for the $s_{t+1}$ (the look-ahead) of the sampled transition tuple. For the online network, we will compute the $Q$-values for the current state $s_t$ and apply action selection using the greedy policy, obtaining the agent estimations for the current state. As explained in section \ref{sec:val-fun_approx}, by minimizing this loss as the TD-error, we are teaching the online network to base the value estimations of the current state $s_t$ using the target network estimations $s_{t+1}$. This could be seen as a "look-ahead" view that glances into the "future" to obtain more information about how good the current state is. Once we got loss value, we adjust the online network weights $\theta$ to minimize the loss using the gradient descent algorithm. 

Finally, when a fixed number of steps has passed, we will copy the online network parameters $\theta$ onto the target network, so that the distribution of the parameters from the target network do not diverge too much from the online network.

\subsubsection{Main loop implementation for DQN}
While the DQN algorithm seems quite straight forward on paper, the implementation tells other story. For doing so, we implemented several modules to compartmentalize the different functionalities. By making the code modular, the process of improving the code, add new functionalities and connect different modules was easier, creating a pipeline that allowed to test several training set-ups in parallel.

The main script is called \href{https://github.com/Javimh18/DL_TFM/blob/main/src/train.py}{train.py} which basically loads all the hyper-parameters that the code needs, such as the discount factor, $\gamma$, for the target estimation or the learning rate when adjusting the net hyper-parameters given the loss. After that, it initializes the agent, that can be of two types, DQN or DDQN to then pass all these arguments into the \href{https://github.com/Javimh18/DL_TFM/blob/main/src/trainer.py}{Trainer} class. This class implements the main functionality explained in algorithm \ref{alg:dqn}, and a code snippet is presented in listing \ref{code:trainer_main_loop}. We will go deeper into this class in the following sections, but for now we will only explain the main functionality. First, we reset the environment, and using the done and truncated flags provided by the agent, we can check whether the episode finishes or not. Then, the agent performs an action according to the action-state values over the environment, obtaining a transition tuple which is stored in the replay memory. Finally, the agent learns from the pool of experiences stored in the replay buffer.

\begin{lstlisting}[caption={Trainer main loop}, label={code:trainer_main_loop}]
while self.curr_step < self.n_steps:
	# reset environment
	done, trunc = False, False
	state = self.env.reset()
	#measure_array = []
	while (not done) and (not trunc):
		# 1. get action for state
		action = self.agent.perform_action(state, self.curr_step)  
		# 2. run action in environment
		next_state, reward, done, trunc, info = self.env.step(action) 
		# 3. collect experience in exp. replay buffer for Q-learning
		self.agent.store_transition(state, action, reward, next_state, done, trunc) 
		# 4. Learn from collected experiences
		q, loss = self.agent.learn(self.curr_step)
		# 5. Update the current state 
		state = next_state
		# 6. Update step value 
		self.curr_step += 1            
		logger.log_step(loss, q)

	if 'episode' in info:
		# episode field is stored in the info dict if episode ended
		logger.log_episode(ep_length=info['episode']['l'], ep_reward=info['episode']['r'],)
		if not(self.curr_episode % self.log_every) :
			logger.record(episode=self.curr_episode, 
									epsilon=self.agent.exploration_rate, 
									step=self.curr_step)
			# log the real reward using episode statistics
		self.curr_episode += 1
	
\end{lstlisting}

\subsubsection{Digging deeper: the DQN agent}
\label{sec:digging_dqn_agent}

Once we have seen the training loop, we are going to delve into the agent functionality (\href{https://github.com/Javimh18/DL_TFM/blob/main/src/agents/dqn_agent.py}{dqn\_agent.py}). 

\begin{lstlisting}[caption={Perform action method from the DQN Agent}, label={code:ex_action}]
@torch.no_grad()
def perform_action(self, state, t):
	# decide wether to exploit or explore
	if np.random.random() < self.exploration_rate:
		action =  np.random.randint(0, self.action_dim)
	else:
		# use of __array__(): https://gymnasium.farama.org/main/_modules/gymnasium/wrappers/frame_stack/
		state = first_if_tuple(state).__array__()
		state = torch.tensor(state, device=self.device).unsqueeze(0)
		q_values = self.net(state, model='online')
		action = torch.argmax(q_values, dim=1).item()
	
	# decrease exploration_rate according to scheduler
	self.exploration_rate = self.exp_scheduler.step(t)
	self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)
	
	return action
\end{lstlisting}

As we have seen in the listing \ref{code:trainer_main_loop}, the first thing that our agent does is execute an action. According to algorithm \ref{alg:dqn}, we perform the action according to the target policy. We can see the code in the listing \ref{code:ex_action} the \inlinecode{perform\_action} method when we pick the action that maximizes the action values. After that, we update the exploration scheduler. The exploration scheduler updates the exploration rate $\epsilon$ for each time-step with the \inlinecode{step}. We have used three different types of functions to do so: exponential, linear and product of exponentials, which we will talk about in section \ref{sec:schedulers}. After the agent selects the action, we store the transition in the replay memory module of the agent using the \inlinecode{store\_transition}. In our case, we opted to use \inlinecode{TensorDictReplayBuffer} module from TorchRL, since it has seamless integration with PyTorch, making it quite easy to use and efficient to run, as we can store the replay memory in the GPU, avoiding for bottlenecks in the training process runtime. Inside, this implementation of the replay buffer stores an array of transition tuples as tensors. When the agent samples from the replay memory, it can be done using mini-batches, making the training process more efficient. In the listing \ref{code:replay_memory_dqn} we define the replay memory buffer, where we can state the size of the memory in terms of transition tuples, the device where the replay memory is stored, the sampler or if the replay memory has a mapping to disk in case we have limited storage resources. Additionally, we can also define the algorithm to follow in the sampling process. In our case, we opted to use prioritized experience replay sampling \cite{schaul2016prioritized} which basically prioritizes more significant experiences based on higher temporal-difference errors during training, enhancing learning. The motivation behind this work is similar to what happens in unbalanced datasets, as the less frequent classes usually are the hardest to recognize for the classifier. In order to mitigate this, different techniques ensure to weight more importance sampling to the less frequent instances in the dataset. 

\begin{lstlisting}[caption={Code snippet for the replay memory of the DQN agent}, label={code:replay_memory_dqn}]
# defining the memory (experience replay) of the agent
self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(
	max_size=float(agent_config['replay_memory_size']),
	scratch_dir='./memmap_dir',
	device=self.device,
	),sampler=PrioritizedSampler(max_capacity=int(float(agent_config['replay_memory_size'])), 
	alpha=1.0, 
	beta=1.0)
)
\end{lstlisting}

Storing different experiences along the exploration stage tries to ensure that by the time the agent starts to learn by back-propagation, there is enough data variability to capture enough different situations in the environment to learn to act in most of them. In our implementation, the learning process is done by the \inlinecode{learn} method from the agent class. We present the function in listing \ref{code:learn_dqn_fun}, where first, we have a series of if statements that check several conditions. The first if statement checks that a fixed number of steps has passed in the training to synchronize the online and target network parameters (i.e. $\theta_{online} \rightarrow \theta_{target}$). After that, we check if a fixed number of training steps has passed to save the state of the network periodically. In order to fill the replay memory buffer before starting to learn from it, we define a number of steps where the network does nothing, called burning, that ensures that, by the time the network starts to adjust its weights, there is enough variability of experiences to learn from. Also, for computational resources purposes, we made the network learn only every \inlinecode{learn_every} steps.

\begin{lstlisting}[caption={\inlinecode{learn} function from the DQN agent}, label={code:learn_dqn_fun}]
def learn(self, step):
	
	# Once the error is computed, each sync_every the weights of the 
	# target network are updated to the online network
	if not (step % self.sync_every):
		self.sync_Q_target()
	
	# save the model each save_every steps
	if not (step % self.save_every) and step > 0:
		self.save(step)
	
	# Burning lets episodes pass but collects experiences for the memory buffer
	if step < self.burning:
		return None, None
	
	# Not learning every step, but every "learn_every" steps
	if step % self.learn_every != 0:
		return None, None
	
	state, action, reward, next_state, done, _ = self.recall()
	
	# once we have our transition tuple, we apply TD learning over our DQN and compute the loss
	q_estimate = self.compute_q_estimate(state, action)
	q_target = self.compute_q_target(reward, done, next_state)
	
	# update the q_online network using the loss
	loss = self.loss_fn(q_target, q_estimate) # Compute Huber loss
	self.optimizer.zero_grad()
	loss.backward()
	self.optimizer.step()
	
	return q_estimate.mean().item(), loss.item()
\end{lstlisting} 

After these checks, we start with the learning process. To do so, we first recall or sample from the replay buffer hat extracts a set of samples with a fixed batch size (i.e. 8, 16, 32, etc)for the model to learn from them. Thanks to tensor operation support provided from PyTorch, this is forwarded through the online and target networks seamlessly. The \inlinecode{compute\_q\_target} method implements the TD target element in the DQN loss equation \ref{eq:dqn_loss} which is $r + \gamma \max_{a'} Q(s', a'; w^-))$, while the method \inlinecode{compute\_q\_estimate} implements the TD estimate which is $Q(s, a; w)$ where actions $a$ and $a'$ are selected according to the target greedy policy for the sampled state s from the buffer and its corresponding next state s'.

With the TD values, we compute the loss. We opted for the Huber Loss \cite{9918637}, defined in equation \ref{eq:hub_loss} where $\delta$ is a regularization parameter. This type of loss is useful in these set-ups, thanks to its robustness to outliers in the data compared to the mean squared error loss function, which is of convenience for this problem in cases where loss between Q-values estimates fall far over the hyper-parameter $\delta$.

\begin{equation}
	\label{eq:hub_loss}
	L_{\delta}(y, f(x)) = 
	\begin{cases} 
		\frac{1}{2}(y-f(x))^2 & \text{for $|y-f(x)| \leq \delta$, } \\
		\delta \cdot \left(|y-f(x)| - \frac{1}{2}\delta \right) & \text{otherwise}
	\end{cases}
\end{equation}

Finally, once the loss is computed, we perform the back-propagation process in the online network to adjust its weights and we return the mean Q-values for the sampled mini-batch.

\subsubsection{From DQN to DDQN}
\label{sec:ddqn_training_loop}
In \cite{vanhasselt2015deep}, Van Hasselt \text{et al.} proposed an updated version of the DQN algorithm. As explained in section \ref{sec:val-fun_approx}, the overestimation bias from the DQN comes from the target evaluating and selecting the actions from the target network. To mitigate this, they propose to decouple the action selection for the TD-target, using the online network and evaluating that action with the target network. We present the pseudocode in Algorithm \ref{alg:ddqn_pseudocode}. As we can see, the only difference between Algorithm \ref{alg:dqn} and Algorithm \ref{alg:ddqn_pseudocode} is the action selection. In Algorithm \ref{alg:ddqn_pseudocode}, the action to evaluate is selected by the online Q-network in line 13, while in Algorithm \ref{alg:dqn}, the action is selected by the offline Q-network in line 13. With these changes, we had to adapt the code consequently to implement this algorithm.

\begin{algorithm}[!h]
	\caption{Double Deep Q-Learning (DDQN)}
	\label{alg:ddqn_pseudocode}
	\begin{algorithmic}[1]
		\State Initialize replay memory $D$ to capacity $N$
		\State Initialize primary action-value function $Q$ with random weights $\theta$
		\State Initialize target action-value function $\hat{Q}$ with weights $\theta^- = \theta$
		\For{episode $= 1, M$}
		\State Initialize state $s_0$
		\For{t $= 1, T$}
		\State With probability $\epsilon$, select a random action $a_t$
		\State Otherwise, select $a_t = \arg\max_a Q(s_t, a; \theta)$
		\State Execute action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$
		\State Store transition $(s_t, a_t, r_t, s_{t+1})$ in $D$
		\State Sample random minibatch of transitions $(s_j, a_j, r_j, s_{j+1})$ from $D$
		\State Set $a' = \arg\max_a Q(s_{j+1}, a; \theta)$
		\State Set $y_j = 
		\begin{cases} 
			r_j & \text{if episode terminates at step } j+1 \\
			r_j + \gamma \hat{Q}(s_{j+1}, a'; \theta^-) & \text{otherwise}
		\end{cases}$
		\State Perform a gradient descent step on $(y_j - Q(s_j, a_j; \theta))^2$ with respect to the network parameters $\theta$
		\State Every $C$ steps, update target network $\theta^- = \theta$
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}


Since the training loop is basically the same for DQN and DDQN, we made the DDQN agent class inherit from the DQN agent in the \href{https://github.com/Javimh18/DL_TFM/blob/main/src/agents/ddqn_agent.py}{ddqn\_agent.py} script, taking advantage of the already developed code. In listing \ref{code:ddqn_action_selection} we see the implementation of the \inlinecode{compute_q_target} method for the DDQN agent.

\begin{lstlisting}[caption={DDQN action selection algorithm}, label={code:ddqn_action_selection}]
@torch.no_grad
def compute_q_target(self, reward, next_state, done):
	# for the next state, get the actions that have higher q_values
	online_q_action_value = self.net(next_state, model='online')
	max_value_action = torch.argmax(online_q_action_value, dim=1)
	# then, apply those actions onto the target (off-line) model
	target_q_action_values = self.net(next_state, model='target')
	q_next_max_value = target_q_action_values[np.arange(0, self.batch_size), max_value_action]
	return (reward + (1-done.float()) * self.gamma * q_next_max_value)
\end{lstlisting}

\section{Q-networks integration}
Q-networks are the essential part from DQN and DDQN algorithms. In this work we have trained three different agents using three different Q-networks: CNN, ViT and SWIN. The CNN will serve as our baseline, since it is the most simple out of the proposed networks, and we used the same architecture as in \cite{mnih2013playing}, which we will use to see which is the performance that our networks should be up to, and hopefully improve.

We first tried to implement the code for the ViT and SWIN transformer from scratch in order to delve into understanding how these models go from intuition to actual implementation, but we face a big drawback.  As we performed some tests, we found out that some existent implementations were already more efficient that ours in computing time. The implementation in our work for the SWIN Transformer was taken from \href{https://github.com/microsoft/Swin-Transformer}{this} repository that is the implementation of \cite{liu2021swin}. On the other hand, the implementation of the Vision Transformer that we used for our work was taken from \href{https://github.com/facebookresearch/dino/blob/main/vision_transformer.py}{here}, that is the implementation of \cite{vit}. If the reader wants a deeper analysis of the code for both the ViT and the SWIN Transformer, they can refer to sections \ref{sec:vit_transformer_imp} and \ref{sec:swin_transformer_rl} from appendix \ref{cha:attn_models_code_analysis}.

To integrate the implementation of these models, we had to enable the Q-network to be an instance of either the CNN, ViT or the SWIN Transformer. This is why we created a class called \inlinecode{DQN\_network}, that encapsulates the three possibilities in just one class. The most important part of the code is defined in listing \ref{code:q_networks_implementation} and the whole code is in the \href{https://github.com/Javimh18/DL_TFM/blob/main/src/agents/dqn_models.py}{dqn\_models.py} file. We can see that the is a argument called \inlinecode{config} that is passed to the constructor. The implementation of this functionality is explained in section \ref{sec:conf_files}, but for we will just say that this variable is a dictionary that contain several values and configurations for the Q-networks topology and the DQN and DDQN agents. 

\begin{lstlisting}[caption={Code for the Q\_network initialization}, label={code:q_networks_implementation}]
	class DQN_network(nn.Module):
		def __init__(self, type:str, n_actions:int, obs_shape:tuple, config:dict) -> None:
			super().__init__()
			nn_config = config[type]
			if type == 'vit':
				self.online = ViT(img_size=obs_shape[-1],
					patch_size=int(nn_config['patch_size']),
					in_chans=obs_shape[0],
					embed_dim=int(nn_config['embed_dim']),
					num_heads=int(nn_config['n_heads']),
					depth=int(nn_config['n_layers']),
					num_classes=n_actions)
				
				self.target = ViT(img_size=obs_shape[-1],
					patch_size=int(nn_config['patch_size']),
					in_chans=obs_shape[0],
					embed_dim=int(nn_config['embed_dim']),
					num_heads=int(nn_config['n_heads']),
					depth=int(nn_config['n_layers']),
					num_classes=n_actions)
					
			elif type == 'swin':
				self.online = SwinTransformer(img_size=obs_shape[-1], 
					patch_size=nn_config['patch_size'], 
					in_chans=obs_shape[0], 
					num_classes=n_actions, 
					embed_dim=nn_config['embed_dim'],
					depths=nn_config['depths'], 
					num_heads=nn_config['num_heads'], 
					window_size=nn_config['window_size'])
				self.target = SwinTransformer(img_size=obs_shape[-1], 
					patch_size=nn_config['patch_size'], 
					in_chans=obs_shape[0], 
					num_classes=n_actions, 
					embed_dim=nn_config['embed_dim'],
					depths=nn_config['depths'], 
					num_heads=nn_config['num_heads'], 
					window_size=nn_config['window_size'])
					
			elif type == 'cnn':
				self.online = CNN(n_actions=n_actions)
				self.target = CNN(n_actions=n_actions)
			else:
				print(f"Type of agent for the model is not correctly specified.\n"
				f"Select between:\n"
				f"\t - Visual Transformer (vit)\n"
				f"\t - SWIN Transformer (swin)\n"
				f"\t - Convolutional Neural Network (cnn)\n"
				f"Exiting..."
				)
				exit()
\end{lstlisting}

Depending on the \inlinecode{type} value from the argument of the constructor, one type of network will be instantiated as the Q-network of the agent. In listing \ref{code:dqn_agent_constructor_q_network} we see a little snippet of the \inlinecode{DQNAgent} constructor, where we can see in line 10 the initialization of the network. Recall from section \ref{sec:ddqn_training_loop} that even though the implementation is on the \inlinecode{DQNAgent}, this functionality is inherited by the \inlinecode{DDQNAgent} class. We remark this, as all the training processes that will be displayed in chapter \ref{cha:evaluation} were performed under the DDQN algorithm, given its superior performance against the DQN algorithm.

\begin{lstlisting}[caption={Part of the constructor method for the \inlinecode{DQNAgent} class}, label={code:dqn_agent_constructor_q_network}]
	class DQNAgent:
		def __init__(self, obs_shape:tuple, action_dim: int, device: torch.device, save_net_dir: str,
			exp_schedule: str, prioritized_replay: bool, agent_config: dict, nn_config:dict):
			
			##########################################
			############ ADDITIONAL CODE ###############
			##########################################
			
			# defining the DQN
			self.net = DQN_network(type=self.type, 
				n_actions=self.action_dim, 
				obs_shape=self.obs_shape,
				config=nn_config).float()
				
			self.net = self.net.to(self.device)
			
			# defining the memory (experience replay) of the agent
			if prioritized_replay:
				sampler = PrioritizedSampler(max_capacity=int(float(agent_config['replay_memory_size'])), 
				alpha=1.0, 
		
\end{lstlisting}

A thing worth mentioning is that we were obliged to train these models completely from scratch, since we could not find pre-trained weights or models in the web. This consumed a lot of our time, as will be explained in section \ref{cha:evaluation}.

\section{Setting up the pipeline}
\label{sec:set_pipeline}

Up until this point, all we have explained in this section is the algorithmic part on how we programmed the agent in order to learn, but that is not the only important thing when developing machine learning algorithms. Our implementation comprises several modules to provide the training framework additional information on how the training process is going, which models work best, and save the weights of the neural networks that help the agent to perform better in the environment. We have already explained in previous sections the main parts which work together to train the different agent configurations that we propose. In this section we will delve into the critical aspects from that have taken part to develop them. 

\subsection{Trainer}
The trainer class is kind of the orchestrator of our code. Its main functionality is to coordinate the environment with the agent learning procedure while extracting relevant metrics and evidences of the agent's behaviour. Our environment has different wrappers added to them, that alter the behaviour and the dynamics of the environment, which may be an issue if the code is not properly adapted. In section \ref{sec:env_dyn} we explain the different types of wrappers that we used, but we have not explained the implications that they have in the way the agent behaves and the metric extraction. For example, our environment uses the \inlinecode{EpisodicLifeEnvironment} which basically treats each life of a game as an episode from the agent to learn, which helps the agent to improve its estimates of the rewards. The issue here is that when we extract the real statistics of the episode they come altered. To solve these problems we make use of the wrapper \inlinecode{RecordEpisodeStatistics} which collects the actual real statistics from the environment. This wrapper adds additional information to the environment when the episode has ended. In listing \ref{code:trainer_info_episode}, that is taken from listing \ref{code:trainer_main_loop}, we can see that in the info dictionary, that is returned for each \inlinecode{env.step(action)} call, there is a key called episode that opens up an embedded dictionary with information that is specific from the episode, such as the accumulated reward or the length of the episode which we extracted and saved for logging purposes. Additional information can be extracted too, such as the last frame that the agent saw.

\begin{lstlisting}[caption={Episodic information retrival from \inlinecode{RecordEpisodeStatistics} wrapper}, label={code:trainer_info_episode}]
if 'episode' in info:
	# episode field is stored in the info dict if episode ended
	logger.log_episode(ep_length=info['episode']['l'], ep_reward=info['episode']['r'],)
	if not(self.curr_episode % self.log_every) :
		logger.record(episode=self.curr_episode, 
		epsilon=self.agent.exploration_rate, 
		step=self.curr_step)
		# log the real reward using episode statistics
		self.curr_episode += 1
\end{lstlisting}

\subsection{Logger}
To record how well an agent is performing, a main performance indicator needs to be tracked: the reward. The issue is that, sometimes, we may encounter environments where the reward gets progressively sparser, occluding information about how well the agent understands the scene and the actions it may perform. As stated in chapter \ref{cha:design-development}, one of the games we are going to develop our evaluation is the MsPacman game. There, rewards are very common at the beginning, but as the game goes on, they become increasingly difficult to obtain, If we do not take this into account, it may seem like the agent is not learning, which may not be true. That is why, additional indicators such as the episode length, the agent's loss or the mean Q-values can be of use to have a better understanding on whether the agent is learning or not. To comprise all this information, we have developed the logger module (\href{https://github.com/Javimh18/DL_TFM/blob/main/src/utils/logger.py}{logger.py}). We must say that for this module, we took inspiration from \cite{feng2022mario}.

The constructor of the logger class (MetricLogger), is defined in listing \ref{code:metric_logger_class}, where we define the main attributes that hold the relevant information that we want to monitor. First, we indicate to the logger if the agent to log is pre-trained and we are continuing the training or performing a fine-tuning or it is a fresh new training thanks to the \inlinecode{load_pre} flag. If so, we assume that in the \inlinecode{save_dir} path there is already information about a previous logged training, if not the a new log file is created under the indicated directory. Additionally, we define the episodic arrays, that hold the information for each episode and the smoothed episodic arrays, that take the moving average of the metrics extracted from the last 100 episodes. After that, we call the \inlinecode{self.init_episode} to indicate the logger that we are going to start logging a new episode.

\begin{lstlisting}[caption={MetricLogger class initialization}, label={code:metric_logger_class}]
class MetricLogger:
	def __init__(self, save_dir:Path, agent_type:str, load_pre=False):
	
	# paths for saving logs and plots
	self.save_log = save_dir / "log"
	if not load_pre:
		# create if not exists
		save_dir.mkdir(parents=True)
		with open(self.save_log, "w") as f:
			f.write(
				f"{'Episode':>8}{'Step':>11}{'Epsilon':>10}{'MeanReward':>15}"
				f"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}"
				f"{'TimeDelta':>15}{'Time':>20}\n"
			)
	else:
		self.load_from_log(save_dir)
		self.save_dir = save_dir
		self.agent_type = agent_type
		
	# History metrics
	self.ep_rewards = []
	self.ep_lengths = []
	self.ep_avg_losses = []
	self.ep_avg_qs = []
	
	# Current episode metric
	self.init_episode()
	
	# Timing
	self.record_time = time.time()
	
	def init_episode(self):
		self.curr_ep_loss = 0.0
		self.curr_ep_q = 0.0
		self.curr_ep_loss_length = 0
\end{lstlisting}

As our agent starts to perform actions, metrics will start to pop up. In order to monitor in the step and in the episode domain, we have two functions \inlinecode{log_step} and \inlinecode{log_episode}, defined in listing \ref{code:log_step_ep}, that compute the step information and record it in the episodic arrays defined in the constructor of the class. This information is stored from the beginning of the training up until the very end, but is not yet dumped into the log file. One thing to notice is that, for the \inlinecode{log_step} function, we only accumulate the values if a loss is provided. If we recall the training procedure, the agent does not learn every step, but every fixed number of steps which helps with the computational over-head. When the agent is not learning, the loss is returned as a \inlinecode{NoneType} object, hence, not recording the step metrics in the logger.

\begin{lstlisting}[caption={Step and episode functions for metrics extraction}, label={code:log_step_ep}]
def log_step(self, loss, q):
	"Mark end of step if loss is provided"
	if loss:
		self.curr_ep_loss += loss
		self.curr_ep_q += q
		self.curr_ep_loss_length += 1
	
def log_episode(self, ep_reward, ep_length):
	"Mark end of episode"
	self.ep_rewards.append(ep_reward)
	self.ep_lengths.append(ep_length)
	
	if self.curr_ep_loss_length == 0:
		ep_avg_loss = 0
		ep_avg_q = 0
	else:
		ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)
		ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)
		self.ep_avg_losses.append(ep_avg_loss)
		self.ep_avg_qs.append(ep_avg_q)
	
	self.init_episode()
\end{lstlisting}

To dump the information into the log file, we defined the \inlinecode{record} function, showed in listing \ref{code:log_record}. First, for all metrics, we compute the moving average of a window of 100 episodic values. This is because usually, training procedures in reinforcement learning (specially in TD learning) holds lots of variability, which sometimes result in noisy results. After that, we print via command line the statistics, for hands-on visualization. At last, we open the log file, and write the computed metrics, to then read it using a \inlinecode{pandas} data-frame, and plot the values using the \inlinecode{plotly} library, which has great integration with pandas data-frames for data plotting.

\begin{lstlisting}[caption={Record function, that saves the moving average statistics in the log file and plots the values of the statistics over the time-steps of the learning}, label={code:log_record}]
def record(self, episode, epsilon, step):
	mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)
	mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)
	mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)
	mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)
	self.steps.append(step)
	
	last_record_time = self.record_time
	self.record_time = time.time()
	time_since_last_record = np.round(self.record_time - last_record_time, 3)
	
	with open(self.save_log, "a") as f:
		f.write(
		f"{episode:8d}{step:10d}{epsilon:10.3f}"
		f"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}"
		f"{time_since_last_record:15.3f}"
		f"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\n"
		)
	df = pd.read_csv(self.save_log, header=0, sep='\s+', skipinitialspace=True)
	for i in df.columns:
		if i not in NOT_2_PLOT:
			fig = px.line(df, x = 'Step', y = i, title=f'{i} over time for {self.agent_type}')
			fig.write_image(self.save_dir / f"{i}.png")
\end{lstlisting} 

The logger is one of the most useful tools that we have developed, since it can manage to monitor the training procedure seamlessly, even when several agents are being trained in parallel and allow us t better visualize the development of the agent's learning. 

\subsection{Schedulers}
\label{sec:schedulers}
In section \ref{sec:digging_dqn_agent}, we talked about how we have proposed three different exploration schemes for the $\epsilon$ value: linear decay, exponential decay and product of exponential decay. To formalize this, we proposed the equations \ref{eq:linear_sch}, \ref{eq:exp_sch} and \ref{eq:prod_exp_sch} for each type of schedule, $\epsilon_t$ is the current exploration rate for a time-step $t$,  $\epsilon_0$ is the initial exploration rate, $\epsilon_f$ is the final exploration rate, $N$ is the number of steps of exploration, $\lambda$ is an hyper-parameter and $\gamma$ is an parameter that we need to infer, given the upper bound $\epsilon_0$ and the lower bound $\epsilon_f$. 

\begin{equation}
	\label{eq:linear_sch}
	\epsilon_t = \frac{\epsilon_f - \epsilon_0}{N} \times t + \epsilon_0
\end{equation}

\begin{equation}
	\label{eq:exp_sch}
	\epsilon_t = \epsilon_0 \gamma_{exp}^{\lambda t}
\end{equation}

\begin{equation}
	\label{eq:prod_exp_sch}
	\epsilon_t = \epsilon_0 \prod_{i=0}^{t} \gamma_{prod}^{\lambda i}
\end{equation}

The $\gamma_{exp}$ value for the exponent decay function is defined in \ref{eq:gamma_exp}, where $\lambda$ is an hyper-parameter, and the rest of the terms involved are the same as in equation \ref{eq:exp_sch}. On the other hand, the $\gamma_{prod}$ value for the product of exponents function is defined in  \ref{eq:gamma_pexp}, where $\lambda$ is an hyper-parameter and $k$ is defined in equation \ref{eq:k_sum} as the general sum of the $N$ first values, where $n$ is the final value, $a_1$ is the first value and $d$ is the interval between the numbers of the sum, in this case 1.

\begin{equation}
	\label{eq:gamma_exp}
	\gamma_{exp} = \sqrt[\lambda N]{\frac{\epsilon_f}{\epsilon_0}}
\end{equation}

\begin{align}
	\gamma_{prod} = \sqrt[\lambda k]{\frac{\epsilon_f}{\epsilon_0}} \quad \label{eq:gamma_pexp}
\end{align}

\begin{equation}
	\label{eq:k_sum}
	k = \sum_{i=0}^{N} i  = n a_1 + d \frac{(n-1)n}{2}
\end{equation}

To clarify that the $\gamma$-values used in these equations is completely different from the discount factor hyper-parameter used in the DQN learning equations defined in \ref{eq:dqn_loss} and \ref{eq:ddqn_loss}. Additionally, the way we came up with them is shown in appendix \ref{app:gamma_proof}. The main goal for defining equation \ref{eq:prod_exp_sch} is to force the exploration rate to be big in the first stages, and decrease with a lower rate than the exponential decay defined in equation \ref{eq:exp_sch}. Figure \ref{fig:shape_exp_funs} visually shows what we have already explained.

To properly scheme the different schedulers, we first defined an abstract class, as seen in listing \ref{code:abs_scheduler}. This class has three main values, the upper bound \inlinecode{e_0}, the lower bound \inlinecode{e_f} and the number of time-steps for the function to go from the upper bound to the lower one. The step method will return the current exploration rate, given the current time-step.
\begin{lstlisting}[caption={Abstract class for the scheduler}, label={code:abs_scheduler}]
class Scheduler(ABC):
	def __init__(self, e_0, e_f, n_steps) -> None:
		self.e_0 = e_0
		self.e_f = e_f
		self.n_steps = n_steps
	@abstractmethod
	def step(self, t):
		pass
\end{lstlisting}

The implementation of the linear schedule in listing \ref{code:lin_scheduler} is pretty straight-forward. For each step, we pass the number of the time-step value, which would be the $t$ value for \ref{eq:linear_sch} and \ref{eq:exp_sch} equations and once we have computed the slope, we use as the y-intercept the upper bound \inlinecode{e_0}. 

\begin{lstlisting}[caption={Linear scheduler class}, label={code:lin_scheduler}]
class LinearScheduler(Scheduler):
	def __init__(self, e_0, e_f, n_steps) -> None:
		super().__init__(e_0, e_f, n_steps)
		self.slope = (e_f - e_0)/n_steps
	
	def step(self, t):
		e_t = self.slope*t + self.e_0
		return max(e_t, self.e_f)
\end{lstlisting}

The exponential decay schedule, displayed in listing \ref{code:exp_decay_scheduler}, is quite similar to the linear schedule. Instead of computing the slope, we compute the \inlinecode{gamma} value according to equation \ref{eq:gamma_exp}, but the rest is practically the same.

\begin{lstlisting}[caption={Exponential scheduler class}, label={code:exp_decay_scheduler}]
class ExpDecayScheduler(Scheduler):
	def __init__(self, e_0, e_f, n_steps) -> None:
		super().__init__(e_0, e_f, n_steps)
		self.gamma = (e_f / e_0)**(1/n_steps)
	
	def step(self, t):
		e_t = self.e_0 * self.gamma**t
		return max(e_t, self.e_f)
\end{lstlisting}

Finally, for the product of exponential decay schedule, which is the implementation of equation \ref{eq:prod_exp_sch}, displayed in listing \ref{code:prod_exp_decay_scheduler}, we had to compute the exploration rate in an efficient way to overcome efficiency problems. To do so, instead of looping through the multiplicative product, which would be computationally expensive, we defined an accumulator that held the values for each iteration, so that, when a new time-step comes in, we only have to multiply the accumulator by the exponent of the gamma to the time-step.

\begin{lstlisting}[caption={Product of exponentials decay schedule}, label={code:prod_exp_decay_scheduler}]
def gauss_sum(a:int, n:int, inc:float):
	return n/2*(2*a+(n-1)*inc)
	
class ProdExpDecayScheduler(Scheduler):
	def __init__(self, e_0, e_f, n_steps) -> None:
		super().__init__(e_0, e_f, n_steps)
	
		k = gauss_sum(a=0, n=n_steps, inc=1.0)
		self.gamma = (e_f / e_0)**(1/k)
		self.acc = e_0
	
	def step(self, t):
		self.acc *= self.gamma**t
		return max(self.acc, self.e_f)
\end{lstlisting}

To have a better understanding on how these equations behave over time, the shape of them is depicted in Figure \ref{fig:shape_exp_funs}, where $\gamma$ is the value computed according to the bounds of our exploration schedule. We came up with these different schedules to explore if they affect the exploration process when the agent is learning over the first steps. We will discuss more about this in Section \ref{cha:evaluation}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/shape_exploration_functions}
	\caption{Shape of the proposed exploration functions for the DQN agent}
	\label{fig:shape_exp_funs}
\end{figure}

\subsection{Extracting explainability features}
\label{sec:extracting_exp_feat}
In this section we are going to explain how, once we trained our models, we extracted several feature maps to understand what the model is taking into account to perform an action. We must note that for the SWIN Transformer, in section \ref{sec:extracting_attn_maps}, we could not implement a way to extract raw attention maps, nor we found any reference in the literature that would help us with this purpose, so we only extracted the attention maps for the ViT model. For extracting the activation maps using established methods in the XAI field in section \ref{sec:grad_cam_act_maps}, the developed method takes into account both ViT and SWIN transformer.

\subsubsection{Extracting attention maps}
\label{sec:extracting_attn_maps}
As explained in section \ref{sec:vis-transformers}, the ViT introduces an additional token along the patches, called the class token, which holds the information that class takes into account via self-attention to produce the output. Figure \ref{fig:classtokenattention} illustrates this. For a 4 by 4 patched image, the disposition of the attention matrix for one attention head would give us for the \textbf{Class Token} row, all the attention weights with respect to the rest of the patches. The code developed for this section is available in the script \href{https://github.com/Javimh18/DL_TFM/blob/main/src/visualize_attention_vit.py}{visualize\_attention\_vit.py}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/class_token_attention}
	\caption{Example of an attention matrix for one attention head with 4 $\times$ 4 patches. The section under the red rectangle refers to the patches that the class token ponders in order to perform the output. This patches will be our extraction for the attention maps.}
	\label{fig:classtokenattention}
\end{figure}

In order to extract the attention maps, the implementation of the ViT used from \cite{caron2021emerging} has the functionality of returning the attention maps in the forward pass of the ViT blocks (see listing \ref{code:attn_block}). The only thing we ought to do, is index the attention matrix correctly to extract the patches of attention with respect to the class token (patches that fall under the red rectangle in Figure \ref{fig:classtokenattention}). In listing \ref{code:single_frame_attn}, we can see an example of this. First, we obtain the attention weights for each patch by indexing the attention matrix like \inlinecode{attn[0, :, 0, 1:]} in line 3, where the first dimension is de batch, the second is the attention heads, the third would be rows and the fourth the columns. This indexing returns a matrix with dimensions $\mathbb{R}^{nh \times (P-1)}$, where nh refers to the number of heads from the attention mechanism, and P refers to the tokens that go into the attention mechanism. Since we are not interested in the attention weight that the class token has with respect to itself, we exclude it out, hence the \inlinecode{:1} in the indexing for the fourth dimension. To compute the averaged importance of each patch, we perform the mean across the nh dimension, and reshape the $P$ dimension to the original spatial dimensions $P_w$ and $P_h$ for the width and the height respectively (line 4). Then, by calling the method \inlinecode{interpolate_attn_map} we perform a interpolation to return the attention map to the original size of the input (lines 5 to 8).

\begin{lstlisting}[caption={Attention map extractor for a single frame}, label={code:single_frame_attn}]
	vit = agent.net.online
	attn = vit.get_last_selfattention(state.float())
	cls_attn = attn[0, :, 0, 1:]
	cls_attn = cls_attn.reshape(-1, dimension, dimension).mean(dim=0, keepdim=True).unsqueeze(0)
	# 3. Interpolate the attention maps to original frame size
	cls_attn_int = interpolate_attn_map(attention_map=cls_attn, 
	scale_factor=patch_size,
	final_shape=original_shape)
\end{lstlisting}



Once we have done this, we can plot the attention map along with the original frame to observe where does the model pay attention to, given a frame. In Figure \ref{fig:attnvisualizationframe} we provide an example for this. Additionally, in the right side of the figure, we provide the returned Q-values from the network, since they are of vital importance for understanding the network's decisions. The interpretations that we extract from this will be discussed in chapter \ref{cha:evaluation}, along with some additional insights for each attention head.

\begin{figure} [!h]
	\centering
	\includegraphics[width=\linewidth]{figures/attn_visualization_frame}
	\caption{Left side, the original frame from the game, in the center, the attention maps for the class token, at the right-side, the Q-values for the observation from the state.}
	\label{fig:attnvisualizationframe}
\end{figure}


\subsubsection{Extracting Grad-CAM's activation maps}
\label{sec:grad_cam_act_maps}
We developed something similar to section \ref{sec:extracting_attn_maps} but using the grad-CAM technique explained in \ref{sec:cam}. To do so, we used the \inlinecode{pytorch-cam} library \cite{jacobgilpytorchcam} that integrates a well varied amount of CAM methods to select. Part of the developed code is displayed in listing \ref{code:grad_cam_rl}, and the complete code can be found in the \href{https://github.com/Javimh18/DL_TFM/blob/main/src/visualize_activations.py}{visualize\_activations.py} script.

\begin{lstlisting}[caption={Activation maps using Grad-CAM}, label={code:grad_cam_rl}]
	action, q_v = agent.perform_action(state, t=-1, exploit=True)
	actions.append(action)
	q_values.append(q_v.squeeze().detach().cpu().numpy())
	# 2. get the activation maps
	state = first_if_tuple(state).__array__()
	state = torch.tensor(np.array([state]), device=device)
	action_target = ClassifierOutputTarget(action)
	gray_scale_cam = cam(input_tensor=state, targets=[action_target])
	gray_scale_cam = torch.tensor(gray_scale_cam[0, :]).unsqueeze(0).unsqueeze(0) # appropiate dims for interpolation
	activ_map = interpolate_activations(gray_scale_cam,
	final_shape=original_shape)
\end{lstlisting}

To adapt Grad-CAM from the classification supervised learning set-up to the reinforcement learning set-up, we treated each of the actions the agent selected as the predicted "class". By doing so, we expected to obtain the biggest activations from the input that the agent considers to perform the output. The \inlinecode{ClassifierOutputTarget} method takes the target class (our action in this case), and passes it to the Grad-CAM method (cam), that returns the activations. Then we perform an interpolation to return the resolution to the original frame size, obtaining a map depicted in Figure \ref{fig:activationmapsvisualization}. We must say that this method works for both ViT and SWIN backbones, along with other non-attention based models, such as CNNs. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{figures/activation_maps_visualization}
	\caption{Left-side, the current frame of the game. In the center, the activation maps from the ViT using grad-CAM. At the right-side, the Q-values provided by the neural network.}
	\label{fig:activationmapsvisualization}
\end{figure}

\subsection{Additional implementations}

\subsubsection{Configuration files}
\label{sec:conf_files}
As explained in sections \ref{sec:vit_transformer_imp} and \ref{sec:swin_transformer_rl}, the implementation of these models is highly parametrizable, since we can state lots of parameters that are involved in the topology of the models. Changing these parameters manually in each file is a burden, since we have to check that the changes are done correctly in the several files involved in running the training process. To tackle this issue, we proposed two files: one for the DQN agent hyper-parameters, and another for the Q-networks hyper-parameters called \href{https://github.com/Javimh18/DL_TFM/blob/main/config/agents_config.yaml}{agents\_config.yaml} and \href{https://github.com/Javimh18/DL_TFM/blob/main/config/agent_nns.yaml}{agent\_nns.yaml}.
For loading this files, we used the \inlinecode{pyyaml} library, that eases a lot the loading and manipulation of yaml files, and used them to properly initialize the corresponding hyper-parameters, such as number of steps for training, the learning rate for the back-propagation and the Q-estimates or the number of layers that our model will have. Thanks to this implementation, we were able to seamlessly change the configuration of the agents, allowing for room to explore different hyper-parameters configurations.

\subsubsection{Checkpoints}
As the training process progresses, the weights of our function approximators will change. There are some cases where the excess of training produces a degradation on the model's performance, ending the training with a sub-optimal model. To change this, we implemented a checkpoint saver in our pipeline, that saves periodically the model's state dictionary, that contains the weights of the model. By doing a snap-shot of the model weights, we have several versions, which will be of use when we run our evaluations of the agents in the environment. Additionally, the checkpoints are of great use if we want to resume a training process. We have also developed adaptations to our code that perform these kinds of operations, and have been of great use when unexpected events (such as lack of computational resources) stopped our training process.

\subsection{Overview}
\label{sec:overview_implementation}

With almost all the components involved in the training pipeline for our agents, in Figure \ref{fig:implementationoverview} we present the diagram of all the modules developed along with their relationships. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{figures/implementationoverview}
	\caption{Implementation overview of all the modules involved in this work}
	\label{fig:implementationoverview}
\end{figure}





