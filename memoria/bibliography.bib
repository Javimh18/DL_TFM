%CAPITULO DE LIBRO
@INCOLLECTION{nwagboso1999,
  author = {Nwagboso, Christopher},
  title = {User focused surveillance systems integration for intelligent transport systems},
  booktitle = {Advanced Video-Based Surveillance Systems},
  publisher = {Springer},
  ear = {1999},
  pages = {8-17}
}

%LIBROS COMPLETOS
@BOOK{brunelli09,
  title = {Template matching techniques in computer vision: theory and practice},
  publisher = {John Wiley \& Sons},
  year = {2009},
  author = {Brunelli, Roberto}
}

%ARTICULOS REVISTAS
@ARTICLE{SanMiguel15,
  author = {SanMiguel, J. C. and Cavallaro, A.},
  title = {Cost-Aware Coalitions for Collaborative Tracking in Resource-Constrained Camera Networks},
  journal = {IEEE Sensors Journal},
  volume = {15},
  number = {5},
  pages = {2657-2668},
  year = {2015}
}

%ARTICULOS CONFERENCIAS/WORKSHOPS
@INPROCEEDINGS{dalal05,
  author = {Dalal, Navneet and Triggs, Bill},
  title = {Histograms of oriented gradients for human detection},
  booktitle = {IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)},
  year = {2005},
  volume = {1},
  pages = {886-893}
}

%RECURSOS ELECTRONICOS
@ELECTRONIC{Watson09,
  author = {Watson, Theodore},
  title = {VideoInput library},
  url = {http://www.muonics.net/school/spring05/videoInput/},
  originalyear = {1.01.2009},
  urldate = {1.06.2014}
}

@article{BRAMLAGE202210,
	title = {Generalized attention-weighted reinforcement learning},
	journal = {Neural Networks},
	volume = {145},
	pages = {10-21},
	year = {2022},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/j.neunet.2021.09.023},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608021003853},
	author = {Lennart Bramlage and Aurelio Cortese},
	keywords = {Self-attention, Decision-making, Value function approximation, Deep reinforcement learning, Representation learning, Feature binding},
	abstract = {In neuroscience, attention has been shown to bidirectionally interact with reinforcement learning (RL) to reduce the dimensionality of task representations, restricting computations to relevant features. In machine learning, despite their popularity, attention mechanisms have seldom been administered to decision-making problems. Here, we leverage a theoretical model from computational neuroscience – the attention-weighted RL (AWRL), defining how humans identify task-relevant features (i.e., that allow value predictions) – to design an applied deep RL paradigm. We formally demonstrate that the conjunction of the self-attention mechanism, widely employed in machine learning, with value function approximation is a general formulation of the AWRL model. To evaluate our agent, we train it on three Atari tasks at different complexity levels, incorporating both task-relevant and irrelevant features. Because the model uses semantic observations, we can uncover not only which features the agent elects to base decisions on, but also how it chooses to compile more complex, relational features from simpler ones. We first show that performance depends in large part on the ability to compile new compound features, rather than mere focus on individual features. In line with neuroscience predictions, self-attention leads to high resiliency to noise (irrelevant features) compared to other benchmark models. Finally, we highlight the importance and separate contributions of both bottom-up and top-down attention in the learning process. Together, these results demonstrate the broader validity of the AWRL framework in complex task scenarios, and illustrate the benefits of a deeper integration between neuroscience-derived models and RL for decision making in machine learning.}
}

@article{LEONG2017451,
	title = {Dynamic Interaction between Reinforcement Learning and Attention in Multidimensional Environments},
	journal = {Neuron},
	volume = {93},
	number = {2},
	pages = {451-463},
	year = {2017},
	issn = {0896-6273},
	doi = {https://doi.org/10.1016/j.neuron.2016.12.040},
	url = {https://www.sciencedirect.com/science/article/pii/S089662731631039X},
	author = {Yuan Chang Leong and Angela Radulescu and Reka Daniel and Vivian DeWoskin and Yael Niv},
	keywords = {reinforcement learning, attention, decision making, computational modeling, MVPA, fMRI, vmPFC, striatum, value, prediction error},
	abstract = {Summary
	Little is known about the relationship between attention and learning during decision making. Using eye tracking and multivariate pattern analysis of fMRI data, we measured participants’ dimensional attention as they performed a trial-and-error learning task in which only one of three stimulus dimensions was relevant for reward at any given time. Analysis of participants’ choices revealed that attention biased both value computation during choice and value update during learning. Value signals in the ventromedial prefrontal cortex and prediction errors in the striatum were similarly biased by attention. In turn, participants’ focus of attention was dynamically modulated by ongoing learning. Attentional switches across dimensions correlated with activity in a frontoparietal attention network, which showed enhanced connectivity with the ventromedial prefrontal cortex between switches. Our results suggest a bidirectional interaction between attention and learning: attention constrains learning to relevant dimensions of the environment, while we learn what to attend to via trial and error.}
}

@Article{Wang2020,
	author={Wang, Hao-nan
	and Liu, Ning
	and Zhang, Yi-yun
	and Feng, Da-wei
	and Huang, Feng
	and Li, Dong-sheng
	and Zhang, Yi-ming},
	title={Deep reinforcement learning: a survey},
	journal={Frontiers of Information Technology {\&} Electronic Engineering},
	year={2020},
	month={Dec},
	day={01},
	volume={21},
	number={12},
	pages={1726-1744},
	abstract={Deep reinforcement learning (RL) has become one of the most popular topics in artificial intelligence research. It has been widely used in various fields, such as end-to-end control, robotic control, recommendation systems, and natural language dialogue systems. In this survey, we systematically categorize the deep RL algorithms and applications, and provide a detailed review over existing deep RL algorithms by dividing them into modelbased methods, model-free methods, and advanced RL methods. We thoroughly analyze the advances including exploration, inverse RL, and transfer RL. Finally, we outline the current representative applications, and analyze four open problems for future research.},
	issn={2095-9230},
	doi={10.1631/FITEE.1900533},
	url={https://doi.org/10.1631/FITEE.1900533}
}

@misc{meng2024deep,
	title={Deep Reinforcement Learning with Swin Transformers}, 
	author={Li Meng and Morten Goodwin and Anis Yazidi and Paal Engelstad},
	year={2024},
	eprint={2206.15269},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{gu2016continuous,
	title={Continuous Deep Q-Learning with Model-based Acceleration}, 
	author={Shixiang Gu and Timothy Lillicrap and Ilya Sutskever and Sergey Levine},
	year={2016},
	eprint={1603.00748},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{chen2021decision,
	title={Decision Transformer: Reinforcement Learning via Sequence Modeling}, 
	author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
	year={2021},
	eprint={2106.01345},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{unlocking_black_box,
	author = {Sankalpa, Nipuna},
	year = {2023},
	month = {06},
	pages = {31-36},
	title = {Unlocking the Black Box: Explainable Artificial Intelligence (XAI) for Trust and Transparency in AI Systems},
	volume = {4},
	journal = {Journal of Digital Art & Humanities},
	doi = {10.33847/2712-8148.4.1_4}
}

@article{beyond_human_DL,
	author = {M. Beatrice Fazi},
	title ={Beyond Human: Deep Learning, Explainability and Representation},
	
	journal = {Theory, Culture \& Society},
	volume = {38},
	number = {7-8},
	pages = {55-77},
	year = {2021},
	doi = {10.1177/0263276420966386},
	URL = {https://doi.org/10.1177/0263276420966386},
	eprint = {https://doi.org/10.1177/0263276420966386},
	abstract = { This article addresses computational procedures that are no longer constrained by human modes of representation and considers how these procedures could be philosophically understood in terms of ‘algorithmic thought’. Research in deep learning is its case study. This artificial intelligence (AI) technique operates in computational ways that are often opaque. Such a black-box character demands rethinking the abstractive operations of deep learning. The article does so by entering debates about explainability in AI and assessing how technoscience and technoculture tackle the possibility to ‘re-present’ the algorithmic procedures of feature extraction and feature learning to the human mind. The article thus mobilises the notion of incommensurability (originally developed in the philosophy of science) to address explainability as a communicational and representational issue, which challenges phenomenological and existential modes of comparison between human and algorithmic ‘thinking’ operations. }
}

@misc{featmap_cnn,
	title = {Applied Deep Learning - Part 4: Convolutional Neural Networks},
	author = {Arden Dertat},
	howpublished = {\url{https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2}},
	note = {Accesed: 31-03-2023}
}

@misc{bahdanau2016neural,
	title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
	author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	year={2016},
	eprint={1409.0473},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{vaswani2023attention,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{xai_cnn,
	author = {Ibrahim, Rami and Shafiq, M. Omair},
	title = {Explainable Convolutional Neural Networks: A Taxonomy, Review, and Future Directions},
	year = {2023},
	issue_date = {October 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {55},
	number = {10},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3563691},
	doi = {10.1145/3563691},
	abstract = {Convolutional neural networks (CNNs) have shown promising results and have outperformed classical machine learning techniques in tasks such as image classification and object recognition. Their human-brain like structure enabled them to learn sophisticated features while passing images through their layers. However, their lack of explainability led to the demand for interpretations to justify their predictions. Research on Explainable AI or XAI has gained momentum to provide knowledge and insights into neural networks. This study summarizes the literature to gain more understanding of explainability in CNNs (i.e., Explainable Convolutional Neural Networks). We classify models that made efforts to improve the CNNs interpretation. We present and discuss taxonomies for XAI models that modify CNN architecture, simplify CNN representations, analyze feature relevance, and visualize interpretations. We review various metrics used to evaluate XAI interpretations. In addition, we discuss the applications and tasks of XAI models. This focused and extensive survey develops a perspective on this area by addressing suggestions for overcoming XAI interpretation challenges, like models’ generalization, unifying evaluation criteria, building robust models, and providing interpretations with semantic descriptions. Our taxonomy can be a reference to motivate future research in interpreting neural networks.},
	journal = {ACM Comput. Surv.},
	month = {feb},
	articleno = {206},
	numpages = {37},
	keywords = {survey, Interpretable AI, convolutional neural networks, Explainable AI}
}

@article{attn_in_video_vigilance,
	author = {Zhao, Yunbin and Zhu, Songhao and Wang, Dongsheng and Liang, Zhiwei},
	year = {2022},
	month = {06},
	pages = {},
	title = {Short range correlation transformer for occluded person re-identification},
	volume = {34},
	journal = {Neural Computing and Applications},
	doi = {10.1007/s00521-022-07400-4}
}

@misc{vit,
	title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
	author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year={2021},
	eprint={2010.11929},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{heuillet2020explainability,
	title={Explainability in Deep Reinforcement Learning}, 
	author={Alexandre Heuillet and Fabien Couthouis and Natalia Díaz-Rodríguez},
	year={2020},
	eprint={2008.06693},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}

@misc{mnih2013playing,
	title={Playing Atari with Deep Reinforcement Learning}, 
	author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	year={2013},
	eprint={1312.5602},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@book{intro_rl,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Reinforcement Learning: An Introduction},
	year = {2018},
	isbn = {0262039249},
	publisher = {A Bradford Book},
	address = {Cambridge, MA, USA},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}

@Article{Watkins1992,
	author={Watkins, Christopher J. C. H.
	and Dayan, Peter},
	title={Q-learning},
	journal={Machine Learning},
	year={1992},
	month={May},
	day={01},
	volume={8},
	number={3},
	pages={279-292},
	abstract={Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	issn={1573-0565},
	doi={10.1007/BF00992698},
	url={https://doi.org/10.1007/BF00992698}
}

@misc{gerstner2021multilevel,
	title={Multilevel Monte Carlo learning}, 
	author={Thomas Gerstner and Bastian Harrach and Daniel Roth and Martin Simon},
	year={2021},
	eprint={2102.08734},
	archivePrefix={arXiv},
	primaryClass={math.NA}
}

@misc{vanhasselt2015deep,
	title={Deep Reinforcement Learning with Double Q-learning}, 
	author={Hado van Hasselt and Arthur Guez and David Silver},
	year={2015},
	eprint={1509.06461},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{schulman2017proximal,
	title={Proximal Policy Optimization Algorithms}, 
	author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	year={2017},
	eprint={1707.06347},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{haarnoja2018soft,
	title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
	author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
	year={2018},
	eprint={1801.01290},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{lillicrap2019continuous,
	title={Continuous control with deep reinforcement learning}, 
	author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	year={2019},
	eprint={1509.02971},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{fu2021d4rl,
	title={D4RL: Datasets for Deep Data-Driven Reinforcement Learning}, 
	author={Justin Fu and Aviral Kumar and Ofir Nachum and George Tucker and Sergey Levine},
	year={2021},
	eprint={2004.07219},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{liu2021swin,
	title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
	author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
	year={2021},
	eprint={2103.14030},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@inproceedings{NEURIPS2022_3908cadf,
	author = {Fu, Yuwei and Wu, Di and Boulet, Benoit},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {8591--8604},
	publisher = {Curran Associates, Inc.},
	title = {A Closer Look at Offline RL Agents},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/3908cadfcc99db12001eafb1207353e9-Paper-Conference.pdf},
	volume = {35},
	year = {2022}
}


@inproceedings{Juozapaitis2019ExplainableRL,
	booktitle={Explainable Reinforcement Learning via Reward Decomposition},
	author={Zoe Juozapaitis and Anurag Koul and Alan Fern and Martin Erwig and Finale Doshi-Velez},
	year={2019},
	url={https://api.semanticscholar.org/CorpusID:204898543}
}

@ARTICLE{model_emsembling_survey,
	author={Mienye, Ibomoiye Domor and Sun, Yanxia},
	journal={IEEE Access}, 
	title={A Survey of Ensemble Learning: Concepts, Algorithms, Applications, and Prospects}, 
	year={2022},
	volume={10},
	number={},
	pages={99129-99149},
	keywords={Boosting;Classification algorithms;Prediction algorithms;Machine learning algorithms;Computational modeling;Bagging;Machine learning;Learning systems;Algorithms;classification;ensemble learning;fraud detection;machine learning;medical diagnosis},
	doi={10.1109/ACCESS.2022.3207287}}

@misc{rl_transformers2023survey,
	title={A Survey on Transformers in Reinforcement Learning}, 
	author={Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
	year={2023},
	eprint={2301.03044},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{vit_q_learning_sample_eff,
	title={Improving Sample Efficiency of Value Based Models Using Attention and Vision Transformers}, 
	author={Amir Ardalan Kalantari and Mohammad Amini and Sarath Chandar and Doina Precup},
	year={2022},
	eprint={2202.00710},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}

@misc{hu2021updet,
	title={UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers}, 
	author={Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},
	year={2021},
	eprint={2101.08001},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{swin_unet,
	author = {Dai, Yin and Xu, Zi'an and Liu, Fayu and Li, Siqi and Liu, Sheng and Shi, Lifu and Fu, Jun},
	year = {2022},
	month = {06},
	pages = {},
	title = {Parotid Gland MRI Segmentation Based on Swin-Unet and Multimodal Images},
	doi = {10.48550/arXiv.2206.03336}
}

@misc{janner2021offline,
	title={Offline Reinforcement Learning as One Big Sequence Modeling Problem}, 
	author={Michael Janner and Qiyang Li and Sergey Levine},
	year={2021},
	eprint={2106.02039},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{wang2022bootstrapped,
	title={Bootstrapped Transformer for Offline Reinforcement Learning}, 
	author={Kerong Wang and Hanye Zhao and Xufang Luo and Kan Ren and Weinan Zhang and Dongsheng Li},
	year={2022},
	eprint={2206.08569},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@online{understandingattentionmechanism,
	author = {Shashank Yadav},
	title = {Understanding Attention Mechanism},
	year = {2023},
	url = {https://shashank7-iitd.medium.com/understanding-attention-mechanism-35ff53fc328e},
	urldate = {2024-04-18},
}

@misc{translationmin2023attention,
	title={Attention Link: An Efficient Attention-Based Low Resource Machine Translation Architecture}, 
	author={Zeping Min},
	year={2023},
	eprint={2302.00340},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@online{markovdecisionprocessgithub,
	author = {Maël Fabien},
	title = {Markov Decision Process},
	year = {2022},
	url = {https://maelfabien.github.io/rl/RL_2/#markov-process-mp},
	urldate = {2024-04-18},
}

@article{Bellemare_2013,
	title={The Arcade Learning Environment: An Evaluation Platform for General Agents},
	volume={47},
	ISSN={1076-9757},
	url={http://dx.doi.org/10.1613/jair.3912},
	DOI={10.1613/jair.3912},
	journal={Journal of Artificial Intelligence Research},
	publisher={AI Access Foundation},
	author={Bellemare, M. G. and Naddaf, Y. and Veness, J. and Bowling, M.},
	year={2013},
	month=jun, pages={253–279} }
	
@article{stable-baselines3,
	author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
	title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
	journal = {Journal of Machine Learning Research},
	year    = {2021},
	volume  = {22},
	number  = {268},
	pages   = {1-8},
	url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{bou2023torchrl,
	title={TorchRL: A data-driven decision-making library for PyTorch}, 
	author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
	year={2023},
	eprint={2306.00577},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@Inbook{Eschmann2021,
	author="Eschmann, Jonas",
	editor="Belousov, Boris
	and Abdulsamad, Hany
	and Klink, Pascal
	and Parisi, Simone
	and Peters, Jan",
	title="Reward Function Design in Reinforcement Learning",
	bookTitle="Reinforcement Learning Algorithms: Analysis and Applications",
	year="2021",
	publisher="Springer International Publishing",
	address="Cham",
	pages="25--33",
	abstract="The reward signal is responsible for determining the agent's behavior, and therefore is a crucial element within the reinforcement learning paradigm. Nevertheless, the mainstream of RL research in recent years has been preoccupied with the development and analysis of learning algorithms, treating the reward signal as given and not subject to change. As the learning algorithms have matured, it is now time to revisit the questions of reward function design. Therefore, this chapter reviews the history of reward function design, highlighting the links to behavioral sciences and evolution, and surveys the most recent developments in RL. Reward shaping, sparse and dense rewards, intrinsic motivation, curiosity, and a number of other approaches are analyzed and compared in this chapter.",
	isbn="978-3-030-41188-6",
	doi="10.1007/978-3-030-41188-6_3",
	url="https://doi.org/10.1007/978-3-030-41188-6_3"
}

@article{exploitation_vs_exploration,
	author = {Sabyasachi Sinha},
	title ={The Exploration–Exploitation Dilemma: A Review in the Context of Managing Growth of New Ventures},
	journal = {Vikalpa},
	volume = {40},
	number = {3},
	pages = {313-323},
	year = {2015},
	doi = {10.1177/0256090915599709},
	URL = { 
	https://doi.org/10.1177/0256090915599709
	},
	eprint = { 
	https://doi.org/10.1177/0256090915599709
	}
}

@misc{schaul2016prioritized,
	title={Prioritized Experience Replay}, 
	author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
	year={2016},
	eprint={1511.05952},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@ARTICLE{9918637,
	author={Patterson, Andrew and Liao, Victor and White, Martha},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={Robust Losses for Learning Value Functions}, 
	year={2023},
	volume={45},
	number={5},
	pages={6157-6167},
	keywords={Approximation algorithms;Optimization;Function approximation;Prediction algorithms;Visualization;Tuning;Time-frequency analysis;Machine learning;reinforcement learning;function approximation},
	doi={10.1109/TPAMI.2022.3213503}
}

@misc{feng2022mario,
	author = {Yuansong Feng and Suraj Subramanian and Howard Wang and Steven Guo},
	title = {Train a Mario-playing RL Agent},
	year = {2022},
	howpublished = {\url{https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html}},
	publisher = {PyTorch.org},
	note = {Accessed: 2024-02-03},
}

@misc{caron2021emerging,
	title={Emerging Properties in Self-Supervised Vision Transformers}, 
	author={Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
	year={2021},
	eprint={2104.14294},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{he2015deep,
	title={Deep Residual Learning for Image Recognition}, 
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2015},
	eprint={1512.03385},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

